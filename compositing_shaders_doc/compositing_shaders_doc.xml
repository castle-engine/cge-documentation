<?xml version='1.0'?>
<!DOCTYPE book PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN"
  "/usr/share/sgml/docbook/dtd/xml/4.5/docbookx.dtd" [
<!-- C++ with zero-width no-break space character, to avoid breaking it
     into two lines (C+ and +). -->
<!ENTITY cpp "C&#xFEFF;+&#xFEFF;+" >

<!-- Don't use larr for now. It looks bad with symbol font
     (increases line height, which is badly visible in the middle of <screen>).
     Making the font smaller doesn't help (at 8pt it still messes line height,
     at 6pt line height is Ok but arrow is too low...). -->
<!-- ENTITY assign "<phrase role='symbol'>&larr;</phrase>"  -->
<!ENTITY assign "<emphasis role='bold'>:=</emphasis>">

<!-- keywords for pseudocode -->
<!ENTITY begin "<emphasis role='bold'>begin</emphasis>">
<!ENTITY end "<emphasis role='bold'>end</emphasis>">
<!ENTITY var "<emphasis role='bold'>var</emphasis>">
<!ENTITY type "<emphasis role='bold'>type</emphasis>">
<!ENTITY procedure "<emphasis role='bold'>procedure</emphasis>">
<!ENTITY foreach "<emphasis role='bold'>for each</emphasis>">
<!ENTITY do "<emphasis role='bold'>do</emphasis>">
<!ENTITY if "<emphasis role='bold'>if</emphasis>">
<!ENTITY then "<emphasis role='bold'>then</emphasis>">
<!ENTITY function "<emphasis role='bold'>function</emphasis>">
<!ENTITY in "<emphasis role='bold'>in</emphasis>">
<!ENTITY and "<emphasis role='bold'>and</emphasis>">
<!ENTITY not "<emphasis role='bold'>not</emphasis>">
<!ENTITY while "<emphasis role='bold'>while</emphasis>">

<!-- common names -->
<!ENTITY glsl "<emphasis>GLSL</emphasis>">
<!ENTITY hlsl "<emphasis>HLSL</emphasis>">
<!ENTITY cg "<emphasis>Cg</emphasis>">

<!-- glsl parameter qualifiers -->
<!ENTITY _inout "<quote>inout</quote>">
<!ENTITY _in "<quote>in</quote>">
<!ENTITY _out "<quote>out</quote>">
<!ENTITY _constin "<quote>const in</quote>">

<!-- figure widths -->
<!ENTITY figure_width_2col "3.0in">
<!ENTITY figure_width      "3.3in">
]>

<!--
  Offline version of this DTD (as installed from Debian testing package) is
  "/usr/share/sgml/docbook/dtd/xml/4.5/docbookx.dtd"
  Online version of this DTD is
  "http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd"
-->

<book>

<bookinfo>
  <title>Compositing Shaders in X3D</title>
  <author>
    <firstname>Michalis</firstname>
    <surname>Kamburelis</surname>
  </author>
  <copyright>
    <year>2011</year>
    <holder>Michalis Kamburelis</holder>
  </copyright>
  <legalnotice><para>You can redistribute and/or modify
    this document under the terms of the
    <ulink url="http://www.gnu.org/licenses/gpl.html">GNU General Public
    License</ulink> as published by
    the Free Software Foundation; either version 2 of the License, or
    (at your option) any later version.</para>
  </legalnotice>
</bookinfo>

<preface id="abstract">
<title>Abstract</title>

<para>We present a new approach for implementing effects using the GPU shading languages.
Our effects seamlessly cooperate with each other and with
the shaders used internally by the 3D application.
Thus the effects are reusable, work in various combinations
and under all lighting and texture conditions.
This makes the GPU shaders more useful for 3D content authors.</para>

<para>Our approach may also be used to integrate internal effects
inside a 3D renderer. Modern renderers need to combine many effects,
like lighting, bump mapping and shadow maps.
As such, it becomes important to develop all these internal effects easily and separately.</para>
<!--
As modern 3D renderers have a number of built-in internal effects
available (like )
This makes the implementation of 3D renderers simpler and cleaner,
as many internal effects
become more independent.</para>
-->

<para>We have designed our effects to fit naturally in 3D scene graph formats,
in particular we present a number of extensions to the X3D standard.
Our extensions nicely integrate shader effects with X3D
concepts like shapes, groups, light sources and textures.
</para>

</preface>

<chapter id="chapter.overview">
<title>Overview</title>

<para>X3D <xref linkend="bib.x3d" /> is an open standard for representing interactive 3D models,
with many advanced graphic features. <xref linkend="chapter.x3d" />
describes X3D in more detail.
</para>

<para>The X3D <emphasis>Programmable shaders component</emphasis> <xref linkend="bib.x3d_shaders" />
(part of the X3D standard) defines how <emphasis>shaders</emphasis> can be assigned
to particular shapes. <!-- and how they interact with other X3D features. -->
<emphasis>Shaders</emphasis> are programs usually executed on the graphic processor unit
(GPU). They control the per-vertex and per-pixel processing,
for example summing the lights contribution
and mixing the texture colors. The authors
can create and assign shaders to shapes, which makes
a myriad of interesting graphic effects possible in X3D models.
<xref linkend="chapter.shaders" /> describes shaders and the standard
way to use them with X3D.
</para>

<para>The shaders designed using the standard nodes
<emphasis>replace</emphasis> the normal rendering functionality, not <emphasis>enhance</emphasis> it.
This reflects the underlying API, like OpenGL or Direct3D.
The 3D libraries, in turn, follow the hardware idea that shader code
should be a complete and optimized program
designed for rendering a particular shape.
</para>

<para>We argue that a different approach is needed in many situations.
Authors usually would like to keep the normal rendering features working
and only add their own effects. The 3D renderer
implementation usually already has an extensive internal shaders system
and the authors want to depend on these internal shaders
to do the common job.
</para>

<para>As an example, consider this simplified lighting equation:</para>

<informalequation>
  <mediaobject>
  <imageobject role="html"> <imagedata format="PNG" fileref="formulas/1.png" /> </imageobject>
  <imageobject role="fo">   <imagedata format="SVG" fileref="formulas/1.svg" /> </imageobject>
  </mediaobject>
</informalequation>

<para>
<!--
The <emphasis>shadow</emphasis> function returns values in the [0..1] range,
scaling the light color. If the object is not in the shadow, it returns 1.
The <emphasis>normal</emphasis> function returns a normal vector at given point.
-->
Different effects want to change various parts of this equation,
without touching the others.
For example, the <emphasis>shadow</emphasis> function may check a shadow map pixel,
or (when shadow map is not available) always return 1.
The <emphasis>normal</emphasis> function may take the vector straight from
the geometry description, or calculate it using a texture value (classic bump mapping).
See <xref linkend="figure.bump_mapping_shadows" />.
The <emphasis>light_color</emphasis> function may be replaced to use different
lighting models (Phong, Ward, Cook-Torrance and so on).
Sometimes it makes sense to change these functions
for all the light sources and sometimes only a specific light source
should behave differently.
Our approach allows you to do everything mentioned here.
<!--
by inserting a piece
of custom shader code into a calculation of a particular function.
on a particular object.
-->
</para>

<para role="figure_2_column">
<figure id="figure.bump_mapping_shadows">
  <title>Japanese shrine model with more and more effects applied.
The model is based on <ulink url="http://opengameart.org/content/shrine-shinto-japan"/>.</title>
  <titleabbrev>Japanese shrine model with more and more effects applied
</titleabbrev>

<informaltable frame="none">
  <tgroup cols="2" align="center">
  <colspec colnum='1' colname='col1' colwidth='1*'/>
  <colspec colnum='2' colname='col2' colwidth='1*'/>

  <tbody>
    <row><entry>
      <mediaobject>
      <imageobject role="html"> <imagedata format="PNG" fileref="images/rhan_shrine_0.png" /> </imageobject>
      <imageobject role="fo">   <imagedata format="PNG" fileref="images/rhan_shrine_0.png" width="&figure_width_2col;" contentwidth="&figure_width_2col;" /> </imageobject>
      </mediaobject>
      No effects.
    </entry>
    <entry>
      <mediaobject>
      <imageobject role="html"> <imagedata format="PNG" fileref="images/rhan_shrine_1_per_pixel_lighting.png" /> </imageobject>
      <imageobject role="fo">   <imagedata format="PNG" fileref="images/rhan_shrine_1_per_pixel_lighting.png" width="&figure_width_2col;" contentwidth="&figure_width_2col;" /> </imageobject>
      </mediaobject>
      Phong shading (per-pixel lighting).
    </entry></row>
    <row><entry>
      <mediaobject>
      <imageobject role="html"> <imagedata format="PNG" fileref="images/rhan_shrine_2_bump_mapping.png" /> </imageobject>
      <imageobject role="fo">   <imagedata format="PNG" fileref="images/rhan_shrine_2_bump_mapping.png" width="&figure_width_2col;" contentwidth="&figure_width_2col;" /> </imageobject>
      </mediaobject>
      Bump mapping.
    </entry>
    <entry>
      <mediaobject>
      <imageobject role="html"> <imagedata format="PNG" fileref="images/rhan_shrine_3_shadow_1st.png" /> </imageobject>
      <imageobject role="fo">   <imagedata format="PNG" fileref="images/rhan_shrine_3_shadow_1st.png" width="&figure_width_2col;" contentwidth="&figure_width_2col;" /> </imageobject>
      </mediaobject>
      1st shadow map.
    </entry></row>
    <row><entry>
      <mediaobject>
      <imageobject role="html"> <imagedata format="PNG" fileref="images/rhan_shrine_4_shadow_2nd.png" /> </imageobject>
      <imageobject role="fo">   <imagedata format="PNG" fileref="images/rhan_shrine_4_shadow_2nd.png" width="&figure_width_2col;" contentwidth="&figure_width_2col;" /> </imageobject>
      </mediaobject>
      2nd shadow map.
    </entry>
    <entry>
      <mediaobject>
      <imageobject role="html"> <imagedata format="PNG" fileref="images/rhan_shrine_5_everything.png" /> </imageobject>
      <imageobject role="fo">   <imagedata format="PNG" fileref="images/rhan_shrine_5_everything.png" width="&figure_width_2col;" contentwidth="&figure_width_2col;" /> </imageobject>
      </mediaobject>
      Both shadow maps.
    </entry></row>
  </tbody>
  </tgroup>
</informaltable>
</figure>
</para>

<para>We present a system for creating effects by essentially compositing
pieces of a shader code. All the effects defined this way effortlessly
cooperate and can be combined with each other and with application internal
shaders. This makes shader programs:</para>

<orderedlist>
  <listitem><para>Much easier to create. We can jump straight into the implementation
    of our imagined algorithm in the shader.
    We are only interested in modifying the relevant shader calculation
    parameter. We do not need to care about the rest of the shader.</para></listitem>

  <listitem><para>Much more powerful. Our effect
    immediately cooperates with absolutely every normal feature of X3D rendering.
    This makes the implemented effect useful for a wide range of real uses,
    not only for a particular situation or a particular model (as it often happens
    with specialized shader code).
    All X3D light sources, textures, even other shader effects,
    are correctly applied.</para></listitem>
</orderedlist>

<para>It is important that we still keep
the full power of a chosen GPU shading language.
We deliberately do not try to invent here a new language, or wrap existing
language in some cumbersome limitations.
This is most flexible for authors,
and it also allows an easy implementation &mdash; there is no need for any complex
shading language processing inside the application.</para>

</chapter>

<chapter id="chapter.x3d">
<title>What is X3D?</title>

<para>X3D <xref linkend="bib.x3d" /> is a language to describe 3D worlds.
The precise specification of the language is open to everyone.
In effect, many applications can handle X3D and cooperate with each other.
For example, you can create an X3D file using any popular 3D modeller
(like <emphasis>Blender</emphasis>) and then load it into any X3D browser
(like our <emphasis>view3dscene</emphasis>, see <xref linkend="bib.castleengine" />).</para>

<para>Many common 3D features, like triangle meshes with materials and textures,
and easy to express. Yet the whole X3D standard is quite large,
including advanced 3D concepts like NURBS,
cube mapping, multi-texturing, particle effects, skinned humanoid animation,
spatial sound, physics.</para>

<para>The scene is represented as a graph of <emphasis>nodes</emphasis>.
In the simple cases, the scene is just a tree of nodes.
In a general case, it can be a directed graph of nodes, with cycles possible.
<!-- Mention DEF/USE keywords. -->
The X3D specification lists the available node types and their fields.
It is also possible to define new full-featured node types using <emphasis>prototypes</emphasis>.</para>
<!-- More about protos?
Prototypes are available
for defining new node types, and external prototypes are available
to load prototypes from other files.
This provides you opportunities to define whole libraries of new objects,
cooperating with each other, much alike libraries and modules
for functions and classes in normal programming languages.
-->

<para>An example of a simple X3D file in
the <quote>classic</quote> encoding follows.
You can save it as a file with extension <literal>.x3dv</literal>
and open with any X3D browser.</para>

<screen>#X3D V3.2 utf8
PROFILE Interchange
Shape {
  geometry Sphere { radius 2 }
}
</screen>

<para>Example below shows the same X3D content encoded using the XML format.
Note that we omitted  <!--the optional (but adviced)-->
DTD and XML schema declarations for brevity.
Again, you can open this file with any X3D browser
(be sure to save it with <literal>.x3d</literal> extension).</para>

<!--
<!DOCTYPE X3D PUBLIC
  "ISO//Web3D//DTD X3D 3.2//EN"
  "http://www.web3d.org/specifications/x3d-3.2.dtd">
<X3D version="3.2" profile="Interchange"
  xmlns:xsd="http://www.w3.org/2001/XMLSchema-instance"
  xsd:noNamespaceSchemaLocation="http://www.web3d.org/specifications/x3d-3.2.xsd">
-->

<screen><![CDATA[<?xml version="1.0" encoding="UTF-8"?>
<X3D version="3.2" profile="Interchange">
  <Scene>
    <Shape>
      <Sphere radius="2" />
    </Shape>
  </Scene>
</X3D>]]>
</screen>

<!-- More about encodings, and binary encoding?
<para>X3D allows to encode the nodes graph in various formats.
The most classic is a text file format, that requires a specialized parser,
and is called nowadays "classic (VRML) encoding". The commonly used now
is an XML encoding. There is also a binary encoding, following standard
methods to binary encode XML.</para>
-->

<para>To enable basic animation and interactive behavior, X3D introduces
the concept of <emphasis>events</emphasis> and <emphasis>routes</emphasis>.
Many nodes have an ability to <emphasis>send events</emphasis>, notifying
about something. There are even special nodes called <emphasis>sensors</emphasis>
with the sole purpose of sending events when something happens.
For example <emphasis>mouse sensors</emphasis>, that report user clicking and dragging
on the scene. Independently, many nodes can also <emphasis>receive events</emphasis>,
which allows to instruct the node to do something (for example,
start the animation).
The X3D author can declare <emphasis>routes</emphasis> that connect
given node's <emphasis>output event</emphasis> (a socket used to send an event) to another node's
<emphasis>input event</emphasis> (a socket used to receive an event).
For example, <quote>open a door
when the handle is pressed</quote>.</para>

<!-- This turns X3D into a declarative programming
language: author merely declares a route between the two sockets,
and the X3D browser will take care to actually propagate the event
and perform appriate action when necessary.</para>
-->

<para>Below is an example of a simple interactive animation.
When you click on a sphere, the <literal>TimeSensor</literal> starts ticking,
which in turn makes the <literal>PositionInterpolator</literal> produce
3D positions with increasing Y value. The positions are then used to move
the sphere up.</para>

<screen><xi:include href="models/simplest_animation.x3dv" parse="text"
  xmlns:xi="http://www.w3.org/2001/XInclude"/></screen>

<!-- Something about scripts?
<para>The behavior of X3D scene can also be enhanced by scripting
languages.</para>
Scripts: integration with script languages (like JavaScript) easy.
Script remains a~``black box'' for~X3D. At the~same time
in X3D we define exactly what parts of the~scene are accessed.
-->

<!--
<para><emphasis>Short history</emphasis>: The predecessor of X3D was called
<emphasis>VRML</emphasis>.
Initial VRML version, <emphasis>VRML 1.0</emphasis>, was based
on a subset of an even older format,
<emphasis>Inventor</emphasis>. Later VRML version, <emphasis>VRML 2.0</emphasis>
(called also <emphasis>VRML 97</emphasis> due to it's release
date), changed the syntax a lot and added many new features for
interactive 3D worlds.
X3D version numbers start from 3.0, to emphasize that X3D is
a natural progression from the VRML. The current version of X3D is 3.2,
with X3D 3.3 in the process of being finalized.
A lot of the software handles both VRML and X3D,
and commonly calls the format just <emphasis>VRML/X3D</emphasis>.</para>
-->

<!--
<para>Programs that interpret and render X3D are traditionally
called <quote>X3D browsers</quote>, as an analogy to <quote>WWW browsers</quote>
and to emphasize the fact that X3D is a dynamic and interactive content.

In our work, we often use the term <quote>3D renderer</quote>,
since we consider most of our invention applicable to any application
rendering any 3D scene graph format.
But in practice, the <quote>3D renderer</quote> that we think
about is the same thing as <quote>X3D browsers</quote> in practice.
-->

</chapter>

<chapter id="chapter.shaders">
<title>Shaders and X3D</title>

<para>Graphic processing unit (GPU) is given a set of 3D points
(vertexes) connected into triangles. For each vertex, some work
should be performed, at least to transform it from an object space
into the clip space (this is when we move and rotate our objects,
and where we can apply perspective projection). <emphasis>Vertex shaders</emphasis>
allow to replace this per-vertex work with a custom program written in a special
<emphasis>shading language</emphasis>.
When the vertexes are processed, the GPU performs <emphasis>rasterization</emphasis>,
determining which screen pixels are actually covered by the triangles.
Then each pixel is drawn, which involves calculating the actual color to be drawn
(for example, mixing color from the lighting calculation with the texture color).
<emphasis>Fragment (pixel) shaders</emphasis> allow to replace this
per-pixel work with a custom program.<!-- <footnote>
<para>The distinction between <emphasis>fragment</emphasis> and <emphasis>pixel</emphasis>
terms is not important for our paper. Usually, <emphasis>one fragment</emphasis>
is exactly the same thing as <emphasis>one pixel</emphasis>.
There are some operations that change this simple correspondence,
but they are not important for our simple introduction to shaders.</para></footnote>
-->
</para>

<para>The most popular real-time shading languages right now are
OpenGL &glsl; <xref linkend="bib.glsl" />,
NVidia &cg; <xref linkend="bib.cg" /> and
Direct 3D &hlsl; <xref linkend="bib.hlsl" />.
They are used for the same purposes and offer practically the same possibilities.
<!--For the sake of this simple introduction, we will not go into discussion
about their differences, advantages and disadvantages against each other.-->
X3D, and our extensions
for compositing shaders described in this paper, support all three of these
languages.</para>

<para>The current implementation of our extensions
supports only the &glsl; (<emphasis>OpenGL Shading Language</emphasis>),
which is probably the most natural to use in an engine based on OpenGL.
As such, most of our examples in this paper will show &glsl;.
</para>

<para>Example &glsl; vertex shader and accompanying fragment shader:</para>

<screen><emphasis>/* vertex shader */</emphasis>
void main(void)
{
  <emphasis>/* pass unchanged texture coordinate to the fragment shader */</emphasis>
  gl_TexCoord[0] = gl_MultiTexCoord0;
  <emphasis>/* calculate vertex position in clip space */</emphasis>
  gl_Position = ftransform();
}
</screen>

<screen><emphasis>/* fragment shader */
/* the texture contents are loaded outside of the shader program,
   by appropriate OpenGL calls. */</emphasis>
uniform sampler2D myTexture;
void main(void)
{
  <emphasis>/* take the texture color at given coordinates, multiply by 2,
     use it to set fragment (pixel) color */</emphasis>
  gl_FragColor = texture2D(myTexture, gl_TexCoord[0].st) * 2.0;
}
</screen>

<para>The shader source code should be processed and passed to the rendering
3D library, like OpenGL, that in turn will pass it to the hardware (GPU).
<!--
Shading languages other than &glsl; may require an additional compilation
step
This is an easy but sometimes
In case of &glsl;, the shader code is just passed directly to the appropriate
OpenGL calls, that take care of preparing it for the execution on GPU.
In case of other shading languages, some additional work may be required
to compile the shader and then pass this compiled version to the final
rendering library.</para>
-->
The complexity of this operation (and the differences between various
shading languages at this step) can be fortunately completely ignored by us.
That is because standard X3D
<emphasis>Programmable shaders component</emphasis>
<xref linkend="bib.x3d_shaders" />
gives us a simple way to attach a shader source code to a 3D shape.
The X3D browser will do all the necessary job of handling the shader
to the underlying libraries and hardware.</para>

<para>A simple working example showing X3D with &glsl; shader code:</para>

<screen>#X3D V3.2 utf8
PROFILE Interchange
Shape {
  appearance Appearance {
    shaders ComposedShader {
      language "GLSL"
      parts ShaderPart {
        type "FRAGMENT"
        url "data:text/plain,
          <emphasis role="bold">void main(void)
          {
            /* just draw the pixel red */
            gl_FragColor = vec4(1.0, 0.0, 0.0, 1.0);
          }</emphasis>"
      }
    }
  }
  geometry Sphere { radius 2 }
}
</screen>

<para>And a little longer example, showing X3D with the previous shader
code (multiplying texture by 2).</para>

<screen>#X3D V3.2 utf8
PROFILE Interchange
Shape {
  appearance Appearance {
    shaders ComposedShader {
      language "GLSL"
      inputOutput SFNode myTexture ImageTexture { url "test_texture.png" }
      parts [
        ShaderPart {
          type "VERTEX"
          url "data:text/plain,
            <emphasis role="bold">void main(void)
            {
              gl_TexCoord[0] = gl_MultiTexCoord0;
              gl_Position = ftransform();
            }</emphasis>"
        }
        ShaderPart {
          type "FRAGMENT"
          url "data:text/plain,
            <emphasis role="bold">uniform sampler2D myTexture;
            void main(void)
            {
              gl_FragColor = texture2D(myTexture, gl_TexCoord[0].st) * 2.0;
            }</emphasis>"
        }
      ]
    }
  }
  geometry Sphere { radius 2 }
}
</screen>

<para>The shader code provided this way <emphasis>replaces</emphasis>
the standard calculations done by the GPU. This means that all
the lighting and texturing effects, if needed, have to be reimplemented
from scratch in our shader. There is no way to combine
our shader with standard rendering features, and it is impossible
to automatically combine two shader sources. This drawback reflects
the design of the hardware. And the whole work presented in this paper
strives to overcome this problem.</para>

<!--
Shader replaces the~calculations, ComposedShader approach, facts:

+ Chooses first applicable shader.
+ You can pass uniform (per-object) and attribute (per-vertex)
  values easily. Like current time, or texture.
+ Easy implementation for renderer, since this is how GPU works,
  this is what OpenGL exposes etc.
- \textbf{Difficult implementation of your own shaders}.
  Before implementing your effects, first learn to recreate
  the~necessary features from fixed-function.
  Sure, you can let some renderer auto-generate a~large shader for you,
  and~only modify it, but...
- \textbf{You create one-time shaders, not reusable}.
  Particular shader is tied to a~scene configuration,
  like the~number and~type of lights and~textures.
  General shader implementing everything
  would be awfully slow, unless you use \texttt{\#ifdefs} &mdash; in which case
  it's awfully large.
- \textbf{All the~effects in one huge shader}.
  Ultimately, this means that you create a~large shader with \texttt{\#ifdefs},
  that is not maintainable. Adding / removing an effect means carefully
  inserting logic into a~large code.

-->

</chapter>

<chapter id="chapter.previous_work">
<title>Motivation and previous work</title>

<para>The popular real-time shading languages
(OpenGL &glsl; <xref linkend="bib.glsl" />,
NVidia &cg; <xref linkend="bib.cg" /> and
Direct 3D &hlsl; <xref linkend="bib.hlsl" />)
do not provide a ready solution for connecting shaders from independent sources.
The <literal>CgFX</literal> and &hlsl; <literal>.fx</literal> files encapsulate shading language code
in <emphasis>techniques</emphasis> (for various graphic card capabilities)
and within a single technique specify operations for each rendering pass.
In neither case can we simply connect multiple shader source code files
and expect the result to be a valid program.</para>

<para>The X3D <emphasis>Programmable shaders component</emphasis> <xref linkend="bib.x3d_shaders" />
makes the three shading languages mentioned above available to X3D authors.
Complete shader code may be assigned to specific shapes.
Although it does not offer any way of compositing shader effects,
<!--
the point of this component
is to expose functionality that maps straightforward to graphic APIs like OpenGL.
-->
this component is still an important base for our work. It defines
how to comfortably keep shader code inside X3D nodes. It also shows
how to pass uniform values (including textures) to the shaders.</para>

<para>An old solution to combine effects, used even before the shading languages,
is a multi-pass rendering. Each rendering pass adds or multiplies
to the buffer contents, adding a layer with desired effect.
However, this is expensive &mdash; in each pass we usually have to repeat
some work, at least transforming and clipping the geometry.
It is also not flexible &mdash; we can only modify
the complete result of the previous pass.
In our work, we want to allow a single rendering pass to be as
powerful as it can.
Arranging shader functions in a pipeline has similar disadvantages
as multi-pass rendering, except there's no speed penalty in this case.</para>

<para>Common approach for writing a flexible shader code is to create
a library of functions and allow the author to choose and compose them
in a final shader to achieve the desired look. But this approach is very limited,
as we cannot modify a particular calculation part without
replicating the algorithm outside of this calculation.
For example, if we want to scale the light contribution by a shadow function,
we will have to also replicate the code iterating over the light sources.</para>

<!--
<para>Another common solution is to arrange shaders in a pipeline, where one
shader processes the result of another. This can be visualized as
layers of materials, where each layer modifies the previous
color. It is actually similar to a multi-pass rendering approach,
except we do not lose speed on repeating the geometry transformation work.
Still it suffers from the same limitations, as we cannot change
the calculation within an existing layer, without replicating the whole
algorithm of this layer. </para>
-->

<!-- (see also http://groups.google.com/group/blendertorenderman/browse_thread/thread/aaf07831b91be9db?pli=1 , confirms my findings) -->

<para>Sh (<ulink url="http://libsh.org/" />, <xref linkend="bib.sh" />)
allows writing shader code (that can run on GPU) directly inside a
&cpp; program.
For this, Sh extends the &cpp; language (through &cpp;
operator overloading and macros tricks).
It allows an excellent
integration between &cpp; code and shaders, hiding the ugly details of
passing variables between normal code (that executes on CPU) and
shader code (that usually executes on GPU). We can use
object-oriented concepts to create a general shader that can
later be extended, for example by overriding virtual
methods. However, this is a solution closely coupled with &cpp;. It's
suitable if we have a 3D engine in &cpp;, we want to use it in
our own &cpp; program and extend its shaders. Our solution is simpler,
treating shader effects as part of the 3D content and can
be integrated into a renderer regardless of its programming language.
We do not need a &cpp; compiler to generate a final GPU shader
and users do not need to be familiar with &cpp;.</para>

<para>OGRE (<ulink url="http://www.ogre3d.org/" />), an open-source 3D engine written in &cpp;, has a system
for adding shader extensions (see <xref linkend="bib.ogre_shader" />). Its idea is similar
to our system (enhance the built-in shaders with our own effects),
however the whole job of combining a shader is done by operating
on particular shader by &cpp; code. The developer has to code
the logic deciding which shaders are extended and most of the specification
about how the extension is called is done in the &cpp; code.
This has the nice advantage of being able to encapsulate some fixed-function
features as well, however the whole system must be carefully controlled by
the &cpp; code.
In our approach, we allow the authors to write direct shading
language code quickly and the integration is built inside appropriate X3D nodes.</para>

<para>AnySL <xref linkend="bib.anysl" /> allows to integrate internal renderer shaders
with user shaders, by introducing a new shader language.</para>
<!--
  We strive to do something more and allow effortless
  integration between many user shaders.
-->

<para>Spark <xref linkend="bib.spark" /> is a recent work presenting a new language to develop
composable shaders for GPU.</para>

<para>In this work, we deliberately decided not to introduce a new shading language.
One of the problems with introducing a new language is that it is
<quote>yet another language to learn</quote> for developers and users.
<!--
Unless a new language becomes a de-facto standard,
obsoleting other languages in the same domain, which is hard.
-->
For developers of 3D rendering applications, there's an additional
effort with integrating the new language with a renderer,
which often is not a trivial task.
This creates a practical problem for new languages &mdash; because
they are not popular, it's even harder for them to become popular.
<!-- Together, this makes adoption of any new language slow. -->
Of course, a new language has also the possibility to introduce new features,
and <quote>win</quote> developers this way.
But we think that existing shading languages,
like &glsl;, are already <!--sufficiently--> comfortable for
a lot of practical purposes.
That's, in a nutshell, our motivation behind <emphasis>extending</emphasis>
an existing shading language, instead of inventing a new one.

<!--
We do not introduce
a new shader language, which is both an advantage (easy implementation,
easier to learn) and disadvantage (we do not hide differences e.g. between
).

  We also try to be simpler as we do not
  introduce any new shading language. On the other hand, the disadvantage
  of our approach is that we do not hide the differences between various shading languages.
  User effects have to be implemented in the same GPU shading language
  that is used by the renderer.
-->
</para>


<para><xref linkend="bib.web3d2010.declarativeshader" /> presents a declarative approach
to an advanced shader in X3D. However, it only allows a fixed set
of functionality, kind of an <emphasis>advanced and enhanced material</emphasis>.
It does not expose any shader functionality to the authors.
It merely allows the authors to use some advanced algorithms that in practice
will be usually implemented by shaders inside the application.
</para>

<para>At the end, we would like to mention a solution from a completely
different domain, that is surprisingly similar to ours in some ways.
Drupal (<ulink url="http://drupal.org/" />),
an open-source CMS system written in PHP,
has a very nice system of modules. Each module
can extend the functionality of the base system (or other module)
by implementing a <emphasis>hook</emphasis>, which is just a normal PHP function
with a special name and appropriate set of parameters. Modules can also define
their own hooks (for use by other modules) and invoke them when appropriate.
This creates a system where it's trivially easy to define new hooks
and to use existing hooks.
Many modules can implement the same hook and cooperate without any problems.
The whole hook system is defined completely in PHP, as it's a scripting
language and we can query the list of loaded functions by name,
and call function by its name.
Drupal approach is quite similar to our
core idea of combining effects. Our effects are similar to
Drupal's modules and our <quote>plugs</quote> are analogous to Drupal hooks.</para>

<para>Our effects can define functions with special names to enhance
the standard shader behavior, just like Drupal modules can define functions
to act on an event from another module.
We can also define new plugs, for other effects to use.
Of course we also have some special problems
(shading language is quite far from a scripting language,
so calling the plugs must be implemented by text replacements)
and some special opportunities (we can define effects at
the appropriate nodes of X3D, like textures and lights sources,
as we do not want to throw all the effects in one bag).</para>

<!--
  OpenSceneGraph:
  http://www.openscenegraph.org/projects/osg/wiki/Support/Tutorials/ShadersParameters
  http://www.openscenegraph.org/projects/osg/wiki/Support/Tutorials/ShadersIntroduction
  http://mew.cx/osg_glsl_july2005.pdf
  I see no way to connect shaders?

  Irrlight &mdash; also no way to connect shaders?

  Blender Game Engine: no way to connect shaders, Python code can
  set the final (complete) shader source only?
  http://download.blender.org/documentation/GE/Blender.htm
  http://www.blender.org/development/release-logs/blender-248/realtime-glsl-materials/
  See also source, inside source/gameengine/:
  ./Ketsji/BL_Shader.h
  ./Ketsji/BL_Shader.cpp
  ./Ketsji/BL_BlenderShader.h
  ./Ketsji/BL_BlenderShader.cpp
-->

</chapter>

<chapter id="chapter.plugs">
<title>Extending the shaders with plugs</title>

<para>The core idea of our approach is that the base shader code defines
points where calls to user-defined functions may be inserted. We call
these places <emphasis>plugs</emphasis>, as they act like sockets where logic
may be added. Each plug has a name and a given set of parameters.
The effects can use special function names, starting with <literal>PLUG_</literal>
and followed by the plug name. These declarations will be found
and the renderer will insert appropriate calls to them from the base shader.</para>

<para>A trivial example of an effect that makes colors two times brighter
is below. This is a complete X3D file, so you can save
it as <literal>test.x3dv</literal> and open with any tool supporting our
extensions, like <literal>view3dscene</literal>.</para>

<screen>#X3D V3.2 utf8
PROFILE Interchange
Shape {
  appearance Appearance {
    material Material { }
<emphasis role="bold">    effects Effect {
      language "GLSL"
      parts EffectPart {
        type "FRAGMENT"
        url "data:text/plain,
        <phrase role="italic">void PLUG_texture_apply(
          inout vec4 fragment_color,
          const in vec3 normal)
        {
          fragment_color.rgb *= 2.0;
        }</phrase>"
      }
    }</emphasis>
  }
  geometry Sphere { }
}
</screen>

<para>Our extensions to X3D are marked with the bold font in the example above.
The &glsl; code inside our extensions is marked with the italic font.
<!--
&glsl; function names starting with <literal>PLUG_</literal> are special,
they will be automatically called at the appropriate place with appropriate
parameters from the final (automatically generated and used) shader.
-->
The special &glsl; function name <literal>PLUG_texture_apply</literal>
indicates that we use the <literal>texture_apply</literal> plug.
This particular plug is called right after applying the textures,
and is the simplest way to <quote>just modify the pixel color</quote>.
<!--
The <literal>texture_apply</literal> plug in
the renderer internal shader. This particular plug,
the <literal>PLUG_texture_apply</literal>, is called after the normal texture colors
are applied, but before the alpha test and is a usual place
-->
<literal>fragment_color</literal> is an &_inout; parameter, by modifying it
we modify the color that will be displayed on the screen.</para>

<para>A reference of all the plugs available in our implementation
is at the end of this paper, see <xref linkend="appendix.plugs" />.
For each plug, like this <literal>PLUG_texture_apply</literal>,
we define a list of parameters
<!-- (they have to be declared exactly the same in an effect), -->
and when it is called.</para>

<para>Many usage scenarios are possible:</para>

<orderedlist>
  <listitem><para>The <literal>Effect</literal> nodes may use plug names
    defined inside the renderer internal shaders. This is the most usual case.
    It allows the authors to extend or override a particular shading parameter.</para></listitem>

  <listitem><para>The <literal>Effect</literal> nodes may also use the plug names defined
    in the previous <literal>Effect</literal> nodes on the same shape.
    It is trivially easy (just add a <quote>magic</quote> comment) to define
    plugs in your own shader code. This way your own effects
    can be customized.</para></listitem>

  <listitem><para>Inside the renderer implementation, the same approach can be used
    to implement some internal effects.
    We have reimplemented many internal effects of our engine,
    like the fog, shadow maps (see our shadow mapping extensions for X3D <xref linkend="bib.castleengine.shadowmaps" />)
    and the bump mapping to use our <quote>plugs</quote> approach.
    This made their implementation very clean, short
    and nicely separated from each other.
    It also proves that
    the authors have the power to implement similar effects easily by themselves.
    </para>

<!--
    <para>
    without any traditional hassles. For example, bump mapping can be
    achieved using standard <literal>ComposedShader</literal> as well,
    but then you have to write all this <quote>boilerplate</quote> code to also deal
    with all the possible lighting and textures configurations.
    Well, the renderer is still useful to calculate nice tangent vectors,
    although an authoring program could generate &glsl; attributes for them as well.</para>

    <para>
    Same thing with shadow maps, they only plug to the <literal>light_scale</literal>
    calculation of appropriate light.</para>
-->
    </listitem>
</orderedlist>

<para>Actually, there are even more possibilities.
We have been talking above about the <quote>renderer internal shaders</quote>,
but the truth is a little more flexible.
When you place a standard shader node
(like a <literal>ComposedShader</literal> node for &glsl; shaders) on the
<literal>Appearance.shaders</literal> list,
then it replaces the internal renderer shaders.
If you define the same (or compatible) plugs inside your shader,
then the internal renderer effects are even added to your own
shader. Of course user effects are added to your shader too.
This way even the standard X3D shader nodes become more flexible.
Note that if you do not define any plugs inside your <literal>ComposedShader</literal> node,
it continues to function as before &mdash; no effects
will be added.</para>

<section id="section.effect_node">
<title>Effect node</title>

<para>New <literal>Effect</literal> node holds information about
the source code and uniform values specific to a given effect.
The node specification below follows the style of
the X3D specification <xref linkend="bib.x3d" />.</para>

<screen><phrase role="underline">Effect : X3DChildNode</phrase>

SFString [] <emphasis role="bold">language</emphasis> ""
  # Language like "GLSL", "CG", "HLSL".
  # This effect will be used
  # only when the base renderer shader
  # uses the same language.

SFBool [in,out] <emphasis role="bold">enabled</emphasis> TRUE
  # Easily turn on/off the effect.
  # You could also remove/add the node
  # from the scene, but often toggling
  # this field is easier for scripts.

MFNode [] <emphasis role="bold">parts</emphasis> [] # EffectPart
  # Source code of the effect.

# A number of uniform values may also be
# declared inside this node.
</screen>

<para>Inside the <literal>Effect</literal> node a number of uniform values may be defined,
passing any X3D value to the shader. Examples include passing
current world time or a particular texture to the shader.
Uniform values are declared exactly like described in the standard
X3D <emphasis>Programmable shaders</emphasis> component <xref linkend="bib.x3d_shaders" />.</para>

<para>The effect source code is split into a number of parts:</para>

<screen><phrase role="underline">EffectPart : X3DNode, X3DUrlObject</phrase>

SFString [] <emphasis role="bold">type</emphasis> "VERTEX"
  # Like ShaderPart.type:
  # allowed values are
  # FRAGMENT | VERTEX | GEOMETRY.

MFString [] <emphasis role="bold">url</emphasis> []
  # The source code, like ShaderPart.url.
  # May come from an external file (url),
  # or inline (following "data:text/plain,").
  # In XML encoding, may also be inlined in CDATA.
</screen>

<para>Inside the effect part source code, the functions that enhance
standard shaders behavior are recognized by names starting with <literal>PLUG_</literal>.
Of course other functions can also be defined and used.
Uniform variables can be passed to the effect,
also varying variables can be passed between the vertex and fragment
parts, just like with standard shader nodes.
</para>

<para>
In a single <literal>EffectPart</literal> node, many <literal>PLUG_</literal>
functions may be declared. However, all plug functions must be declared in the appropriate
effect type. For example, the <literal>texture_apply</literal> plug cannot be used
within a <literal>VERTEX</literal> shader.
If the effect requires some processing per-vertex and some per-fragment,
it is necessary to use two <literal>EffectPart</literal> nodes, with different types.
This allows to implement our system for
<!--
While this may seem like an arbitrary limitation,
this reflects how shader parts are declared in
-->
shading languages with
separate namespaces for vertex and fragment parts (like &glsl;).
A single part may declare many variables and functions,
but it must be completely contained within a given shader type.
</para>

<para>Note that it is completely reasonable to have an <literal>EffectPart</literal> node
with source code that does not define any <literal>PLUG_xxx</literal> functions.
Such <literal>EffectPart</literal> node may be useful for defining shading language
utility functions, used by other effect parts.
</para>

<para>For shading languages that have separate compilation units
(like the <emphasis>OpenGL Shading Language</emphasis>) the implementation may choose to place
each effect part in such separate unit. This forces the shader code to be
cleaner, as you cannot use undeclared functions and variables from other parts.
It also allows for cleaner error detection (parsing errors will be detected
inside the given unit).
</para>

<!--
<para>
We have also considered and rejected a different approach to use plugs.
In that approach, the plug name was indicated by a new field of
the <literal>EffectPart</literal> node, not by a special <literal>PLUG_</literal> function name
inside the shader code.
However, the implementation may need to know the full declaration
of a plug function, to make a forward or external declaration of it.
One way to overcome this problem was to require repeating this declaration
in another field. Another solution was to split shader code into more
parts (for example, one part declares the uniform variables, one part declares
the plug function, one part defines the function body and so on).
Both approaches seemed uncomfortable for authors
and they didn't really offer a simpler implementation, so we dropped this idea.
The current approach, to find the declarations of <literal>PLUG_xxx</literal> functions
inside a complete shader code, is easy to implement and results in clean
shader code of the effects. It also allows us to naturally use
the separate compilation units in case of &glsl;.</para>

<para>
Detecting special <literal>PLUG_</literal> function names inside the shader code
is trivial, and it turned out to be the best approach to design our effects.
This means that each effect code is clean, and can be compiled separately
from others. It allows us to naturally use
the <emphasis>separate compilation units</emphasis> in case of &glsl; implementation.</para>
-->

</section>

<section id="section.effects_on_shape">
<title>Effects for particular shapes</title>

<para>There are various places where an <literal>Effect</literal> node may be used.
To apply an effect for a given shape appearance, it can be placed
on the new <literal>Appearance.effects</literal> list:</para>

<screen><phrase role="underline">Appearance</phrase>

MFNode [] <emphasis role="bold">effects</emphasis> [] # Effect
</screen>

<para>All the effects on this list (with suitable language) will be used.
<!--
Note that this is different
than the <literal>Appearance.shaders</literal>, which chooses only one shader.
For effects, we choose all of them.
-->
This allows authors to define a library of independent shader effects
and then trivially pick desired effects for each particular shape.
Simply placing two effects on the <literal>Appearance.effects</literal> list
makes them cooperate correctly.
<!--
. This also allows to
define a library of effects, that can be composited without any work
needed by user.
--></para>

<figure id="figure.toon_and_fresnel">
  <title>Toon and Fresnel effects combined</title>
  <mediaobject>
  <imageobject role="html"> <imagedata format="PNG" fileref="images/fresnel_and_toon.png" /> </imageobject>
  <imageobject role="fo">   <imagedata format="PNG" fileref="images/fresnel_and_toon.png" width="&figure_width;" contentwidth="&figure_width;" /> </imageobject>
  </mediaobject>
</figure>

<para>Note that all introduced nodes benefit from X3D mechanism to reuse the nodes
by reference (the <literal>DEF</literal> / <literal>USE</literal> keywords). Reusing the
<literal>Effect</literal> nodes
is most natural and allows to combine existing effects in any desired way.
Reusing the <literal>EffectPart</literal> nodes is also useful, when some effects
would like to share a particular piece of code. For example,
the same <literal>EffectPart</literal> node, with a library of useful
shading language functions, may be used for many effects.</para>

</section>

<section id="section.effects_on_group">
<title>Effects for a group of nodes</title>

<para>The <literal>Effect</literal> node is a descendant of the abstract <literal>X3DChildNode</literal>.
As such it can be placed directly within X3D grouping nodes like
<literal>Group</literal>, <literal>Transform</literal> and at the top level of the X3D file.
Such effect will apply to all the shapes within the given group.
The scope rules follow the X3D conventions for other nodes,
like pointing device sensor nodes and <literal>LocalFog</literal>.</para>

<para>The <literal>LocalFog</literal> example is worth emphasizing. Using our system,
an X3D viewer can implement the <literal>LocalFog</literal> node as a prototype
that expands to our <literal>Effect</literal> node. This results in a 100% correct
and easy implementation of the standard <literal>LocalFog</literal> node.</para>

<para>As one of the demos, we have implemented a realistic
animated volumetric fog, where the fog density is stored in
a 3D smooth noise texture (idea from <xref linkend="bib.humus.volumetricfog" />).
In a fragment shader, the 3D&nbsp;texture is sampled
along the line between the camera and pixel position in the 3D&nbsp;space. This makes a very
convincing effect of a dense fog. The <literal>Effect</literal> node with
appropriate shader code is placed at the top level of the X3D file,
so it simply works for all shapes. See <xref linkend="figure.fog" />.</para>

<para role="figure_2_column">
<figure id="figure.fog">
  <title>Volumetric fog with animated density</title>

<informaltable frame="none">
  <tgroup cols="2" align="center">
  <colspec colnum='1' colname='col1' colwidth='1*'/>
  <colspec colnum='2' colname='col2' colwidth='1*'/>

  <tbody>
    <row><entry>
      <mediaobject>
      <imageobject role="html"> <imagedata format="PNG" fileref="images/volumetric_animated_fog_no_fog.png" /> </imageobject>
      <imageobject role="fo">   <imagedata format="PNG" fileref="images/volumetric_animated_fog_no_fog.png" width="&figure_width_2col;" contentwidth="&figure_width_2col;" /> </imageobject>
      </mediaobject>
      No fog.
    </entry>
    <entry>
      <mediaobject>
      <imageobject role="html"> <imagedata format="PNG" fileref="images/volumetric_animated_fog_no_light.png" /> </imageobject>
      <imageobject role="fo">   <imagedata format="PNG" fileref="images/volumetric_animated_fog_no_light.png" width="&figure_width_2col;" contentwidth="&figure_width_2col;" /> </imageobject>
      </mediaobject>
      No lighting.
      Note that the fog is assumed to have its own ambient lighting,
      so it colors the image even in this case.
    </entry></row>
    <row><entry>
      <mediaobject>
      <imageobject role="html"> <imagedata format="PNG" fileref="images/volumetric_animated_fog_all.png" /> </imageobject>
      <imageobject role="fo">   <imagedata format="PNG" fileref="images/volumetric_animated_fog_all.png" width="&figure_width_2col;" contentwidth="&figure_width_2col;" /> </imageobject>
      </mediaobject>
      Lights and fog.
    </entry>
    <entry /></row>
  </tbody>
  </tgroup>
</informaltable>
</figure>
</para>

</section>

<section id="section.effects_on_lights">
<title>Light sources effects</title>

<para>The nice feature of our system is that effects can be attached to various
types of objects, not just shapes. For example a particular light source
may have a shader effect assigned.
This allows to modify the contribution of a given light.
For example the spot light shape can be modified, possibly
based on some texture information (see <xref linkend="figure.fancy_spot" />).
Or a different lighting model may be implemented, like anisotropic Ward
or Cook-Torrance.
<!--
<para>,
or even replace the default
calculation of the light source contribution. In the second case,
the default calculation will not even be used in the shader,
so it will not slow down the calculation without a reason.</para>
Removed: replacing not really implemented now, not needed.
-->
To make this possible, the <literal>effects</literal> field is added to every light node:</para>

<screen><phrase role="underline">X3DLightNode</phrase>

MFNode [] <emphasis role="bold">effects</emphasis> [] # Effect
</screen>

<figure id="figure.fancy_spot">
  <title>Textured spot light with shadow</title>
  <mediaobject>
  <imageobject role="html"> <imagedata format="PNG" fileref="images/fancy_light_spot_shape.png" /> </imageobject>
  <imageobject role="fo">   <imagedata format="PNG" fileref="images/fancy_light_spot_shape.png" width="&figure_width;" contentwidth="&figure_width;" /> </imageobject>
  </mediaobject>
</figure>

</section>

<section id="section.effects_on_texture">
<title>Texture effects</title>

<para>Just like the light sources, also each texture node may define its own effects:
</para>

<screen><phrase role="underline">X3DTextureNode</phrase>

MFNode [] <emphasis role="bold">effects</emphasis> [] # Effect
</screen>

<para><literal>X3DTextureNode</literal> is an ancestor for all the standard texture nodes,
like the <literal>ImageTexture</literal>. This allows to modify any X3D texture
by shader effects.
A plug <literal>texture_color</literal> may be used to change the texture color,
taking into account the current texture coordinates and other information.</para>

<section id="section.procedural_textures">
<title>Procedural textures</title>

<para>A new X3D node <literal>ShaderTexture</literal> is available for creating
procedural textures using the GPU shading languages.
<!--
This is suitable
if the texture is defined completely using the shading language.
-->
The texture contents are not stored anywhere (not even on GPU)
and the renderer does not manage any texture resources.
From a GPU point of view, there is no texture.
<!-- <footnote>But the spoon is real, we swear.</footnote> -->
There is only a shader function that generates colors
based on some vectors. By wrapping such function inside
the <literal>ShaderTexture</literal> node, it can be treated exactly like other textures
in the scene. In particular, texture coordinates
(explicit or generated) can be comfortably provided
for the procedural texture.
Effectively, it behaves like a normal texture node, with all the related
X3D features.</para>

<para>The new texture node specification:</para>

<screen><phrase role="underline">ShaderTexture : X3DTextureNode</phrase>

MFNode [] <emphasis role="bold">effects</emphasis> [] # Effect

SFString [] <emphasis role="bold">defaultTexCoord</emphasis> "BOUNDS2D"
  # ["BOUNDS2D"|"BOUNDS3D"]
</screen>

<para>Actually, the <literal>effects</literal> field is already defined in
the base <literal>X3DTextureNode</literal> class mentioned previously.
It is repeated here only for completeness.</para>

<para>An effect overriding the <literal>texture_color</literal> plug
should be included, otherwise texture colors are undefined.
Our implementation
sets the default texture color to pink (RGB(1,&nbsp;0,&nbsp;1)), so it stands out,
reminding author to override it.</para>

<para>The texture coordinates, or the algorithm to generate them,
can be explicitly specified, just like for any other texture in X3D.
This is done by placing any <literal>X3DTextureCoordinateNode</literal>
node inside the geometry <literal>texCoord</literal> field.
Both explicit texture coordinate lists (<literal>TextureCoordinate</literal>,
<literal>TextureCoordinate3D</literal>, <literal>TextureCoordinate4D</literal>)
as well as the coordinate generator nodes
(like <literal>TextureCoordinateGenerator</literal> and
<literal>ProjectedTextureCoordinate</literal>) are allowed.
Note that projective texture mapping
by the <literal>ProjectedTextureCoordinate</literal>
is also our X3D extension, see <xref linkend="bib.castleengine.projectivetexturing" />.</para>

<para>When the texture coordinates are not given,
the <literal>defaultTexCoord</literal> field determines how they are generated:</para>

<orderedlist>
  <listitem><para><literal>"BOUNDS2D"</literal> generates 2D texture coordinates,
    adapting to the two largest bounding box sizes. The precise behavior
    of <literal>"BOUNDS2D"</literal> follows the X3D <literal>IndexedFaceSet</literal> specification.</para>

    <para>This is most comfortable
    when the texture color depends only on the XY components of the texture coordinate.
    The 3rd texture coordinate component is always 0,
    and the 4th component is always 1.</para></listitem>

  <listitem><para><literal>"BOUNDS3D"</literal> generates 3D texture coordinates.
    The texture coordinates are adapted to all three bounding box sizes,
    precisely following X3D specification section
    <emphasis>"Texture coordinate generation for primitive objects"</emphasis>
    of the <emphasis>Texturing3D</emphasis> component.</para>

    <para>This is most suitable for true 3D textures. The 4th texture coordinate
    component can be ignored. Or the 4D vector may be treated as homogeneous,
    as <literal>"BOUNDS3D"</literal> will always
    set the 4th component to 1.</para></listitem>
</orderedlist>

<para>The <literal>"BOUNDS*"</literal> names are consistent
with another extension of our engine. We allow
the same values to be used in the <literal>TextureCoordinateGenerator.mode</literal> field.
See <xref linkend="bib.castleengine.texcoordbounds" />.</para>

<para>In the end, the idea is that using a <literal>ShaderTexture</literal>
should be as comfortable as any other texture node.</para>

</section>

<section id="section.when_use_shader_texture">
<title>When to use the ShaderTexture</title>

<para>For textures other than the <literal>ShaderTexture</literal>,
when the <literal>texture_color</literal> plugs are called,
the internal shaders have already calculated the initial texture
color by actually sampling the texture image. This is useful if you
want to modify this color. If you'd rather ignore the normal
sampled color, and always override it with your own, consider using
the special <literal>ShaderTexture</literal> node instead. Using
a normal texture node (like <literal>ImageTexture</literal>) for this
would be uncomfortable, as you would have to load a dummy texture image,
and the shaders could (depending on optimization) waste some time
on calculating a color that will be actually ignored later.</para>

<para>Note that in all cases (effects at <literal>ImageTexture</literal>,
at <literal>ShaderTexture</literal>, etc.) you can always use additional
textures inside the effect. Just like inside a standard <literal>ComposedShader</literal>,
you can declare an <literal>SFNode</literal> field inside an <literal>Effect</literal>
to pass any texture node to the shader as a uniform value.
This allows to combine any number of textures inside an effect.
The only difference
between <literal>ShaderTexture</literal> and other textures is what the system
does automatically for you, that is what color is passed
to the first <literal>texture_color</literal> plug.</para>

<figure id="figure.shader_texture_edge_detection">
  <title>ShaderTexture doing an edge detection operation on a normal ImageTexture</title>
  <mediaobject>
  <imageobject role="html"> <imagedata format="PNG" fileref="images/shader_texture_edge_detection.png" /> </imageobject>
  <imageobject role="fo">   <imagedata format="PNG" fileref="images/shader_texture_edge_detection.png" width="&figure_width;" contentwidth="&figure_width;" /> </imageobject>
  </mediaobject>
</figure>

</section>

<section id="section.filtering">
<title>Texture resolution doesn't matter</title>

<para>The shader effects for textures are calculated at each screen fragment,
not at each texel. So the effects are not concerned with the texture size
or texture filtering options. The <literal>texture_color</literal> plug
receives the interpolated texture coordinates.
<xref linkend="figure.shader_texture_no_filtering_problems" />
shows a blue arc drawn on a texture by our effect.
The arc border is perfectly smooth, without any concern about
the pixel resolution of the underlying texture.
</para>

<figure id="figure.shader_texture_no_filtering_problems">
  <title>Texture effects are not concerned with the texture resolution</title>
  <mediaobject>
  <imageobject role="html"> <imagedata format="PNG" fileref="images/shader_texture_no_filtering_problems.png" /> </imageobject>
  <imageobject role="fo">   <imagedata format="PNG" fileref="images/shader_texture_no_filtering_problems.png" width="&figure_width;" contentwidth="&figure_width;" /> </imageobject>
  </mediaobject>
</figure>

</section>

</section>

</chapter>

<chapter id="chapter.custom_plugs">
<title>Defining custom plugs</title>

<para>In a shader code, new plug may be defined by a magic comment:</para>

<screen>/* PLUG: name (param1, param2, ...) */
</screen>

<para>This defines a point where calls to user functions declared as
<literal>PLUG_name</literal> will be inserted. They will be called with given
parameters.</para>

<para>Many effects may use the same <literal>PLUG_name</literal>.
Even within a single effect, the same <literal>PLUG_name</literal> may be used
many times. All the <literal>PLUG_name</literal> functions
will be uniquely renamed to not collide with each other.</para>

<para>The calls will be added in the order they are specified on the
<literal>effects</literal> list. More precisely, the most local effects
(at light sources and textures) are called first, then the effects
at shape appearance, and finally the effects inside the grouping nodes.
Although, preferably, for most effects this order will not matter.</para>

<para>A plug is often defined to allow modifying some parameter
repeatedly (like adding or modulating the fragment color),
so one or more of the parameters are often allowed to be handled
as &_inout; values.</para>

<para>The same plug name may be defined many times in the source shader.
This means that a single <literal>PLUG_xxx</literal> function will be called
many times. For example, this is useful when the algorithm is naturally
expressed as a loop, but it had to be unrolled for shader source
(for example, to slightly tweak some loop iterations).
The plug names that are available per-light source and per-texture
are an example of this.
<!--
If you use the <literal>PLUG_texture_color</literal>
inside <literal>Appearance.effects</literal>, you change the color of all
the textures (even shader textures).
Removed: Not really available, as params to texture_color differ.
-->
Using the <literal>PLUG_light_scale</literal>
inside <literal>Appearance.effects</literal>, the intensity
of all the light sources on the given shape can be changed. Contrast this with using
the same <literal>PLUG_light_scale</literal> inside a <literal>X3DLightNode.effects</literal>,
where only the given light node contribution is changed.</para>

<para>Currently all the plugs must be procedures, that is their result type
must be declared as <literal>void</literal>. We have been considering
a possibility of functions, where part of the calculation may be replaced
by a call to a plugged function. While not difficult to implement,
this idea seems unnecessary after many tests.
Procedural plugs are easier to declare, as the call to the plug
may be simply inserted, while in case of function it will have to replace
some previous code. This also means that using a procedural plug
<emphasis>never</emphasis> replaces or removes some existing code, which is a very nice
concept to keep. We want the effects to cooperate with each other,
not to <quote>hijack</quote> from each other some parts of the functionality.</para>

<para>New plugs can be defined inside the <literal>Effect</literal> nodes,
as well as inside the complete shaders (like standard
<literal>ComposedShader</literal> nodes).
In the first case, the plugs
are only available for the following effects of the same shape.</para>

<para>The advantage of using magic comments to define plugs is that
they can be ignored and a shader source remains valid.
This means that <literal>ComposedShader</literal> nodes can define custom plugs
and still work (although with no extra effects) even in X3D browsers
that do not support our extensions.</para>

<section id="section.forward_declarations">
<title>Forward declarations</title>

<para>Suppose we have an effect&nbsp;<emphasis>X</emphasis> that defines a new plug,
by including a magic <literal>/* PLUG: ... */</literal> comment.
When this plug is used by another effect&nbsp;<emphasis>Y</emphasis>,
then an appropriate function call is automatically inserted into the generated shader.
In the middle of the source code of effect&nbsp;<emphasis>X</emphasis>,
a function defined in effect&nbsp;<emphasis>Y</emphasis> has to be called. This is the simplest
implementation of our plugs.</para>

<para>Additionally, a forward or external declaration of the called function
may need to be inserted into the effect&nbsp;<emphasis>X</emphasis>. That is because&nbsp;<emphasis>Y</emphasis>
may be in a separate compilation unit (in case of &glsl;),
or just defined lower in the code. In simple cases, such forward or external
declarations can be inserted right at the beginning of effect&nbsp;<emphasis>X</emphasis> code.
</para>

<para>Some shading language directives are required to be placed before
all normal declarations. For example, in case of the <emphasis>OpenGL shading language</emphasis>,
the <literal>#version</literal> as well as some <literal>#extension</literal> directives
must occur at the beginning of the shader code.
To handle such cases, another magic comment <literal>/* PLUG-DECLARATIONS */</literal>
is available.
If present, it signifies a place where forward or external declarations
should be inserted.</para>

<!--
These (forward or external) declarations are inserted at
the point of special <literal>/* PLUG-DECLARATIONS */</literal>
comment, or (when it is missing) simply at the beginning of shader source.
This applies in the same way to shader code inside an <literal>EffectPart</literal>,
or inside standard X3D node like <literal>ShaderPart</literal>.

<literal>/* PLUG-DECLARATIONS */</literal> should be placed after such directives
and before any <literal>/* PLUG: ... */</literal> declarations.
-->

</section>

<section id="section.invalid_shader_code">
<title>Invalid shader code</title>

<para>The behavior is defined only if the provided shading language code
is a correct, self-contained code. The errors (like unterminated block)
<!-- or an unterminated comment) -->
may only be detected after the complete shader
is determined and compiled by the GPU.
It should be noted that for shading languages with separate compilation units,
the parsing errors can be at least reported always for the correct code piece
(effect part).</para>

<para>An invalid effect code may disable
all other user effects on the given shape. That is because
there's no reliable way to detect which user effect prevents the compilation.
At least for shading languages
without the <emphasis>separate compilation units</emphasis> feature.
In such case, the application may decide to disable <emphasis>all</emphasis>
user-provided effects for a given shape.
However, this isn't exactly a new problem &mdash; bad shader code
may always cause enough trouble to prevent the shape from being sensibly rendered.</para>

<!--
<para>It also isn't a security problem &mdash;
X3D <literal>ComposedShader</literal> node allows users
to execute any shading language code anyway. So if there's anything dangerous
(for example a buggy OpenGL may cause the browser process to exit with
segmentation fault on some special shader code snippets),
it could be done without using our effects as well.
This is the same security challenge as the new WebGL is facing &mdash;
some bugs in 3D libraries (like OpenGL) become security problems when
the file author has flexibility to (almost directly) issue OpenGL
calls or provide custom shader code.</para>
-->

<!--
<para>In all our practical tests, this approach didn't cause any problems.
Since each effect can be implemented separately, it can also be tested separately,
and in practice it's usually obvious where to look for the parsing error.</para>
-->

<!--
<para>It may be noted that in nasty cases,
a deliberately poorly coded effect may cause troubles for other effects.
In particular, since you can use <literal>#define</literal> and macros in your effect code,
you can do nasty tricks to break other effects. You can make them compile,
but function incorrectly. However, we do not consider
it a real problem. You really have to deliberately want to do something bad,
and be familiar with internals about how the shader is generated,
to achieve some particular weird behavior.
It does not happen by accident in our experience.
Moreover, you may need to use the internal knowledge
how the other effects are implemented (maybe how they are implemented
inside the browser).</para>
-->

<para>The <emphasis role="bold">application does not need to parse the shader code</emphasis> at any point.
Only a trivial text search in the code is necessary to detect the magic
plug function names and comments.
<!-- Still, an incorrect effect code may cause the whole shader to malfunction. -->
</para>

</section>
</chapter>

<chapter id="chapter.examples">
<title>Examples</title>

<para>A static screenshot will never express the freedom
of movement in an animated 3D scene. So we would like to encourage
the reader to try the examples mentioned in this chapter yourself.
Download <literal>view3dscene</literal>, our X3D browser,
from <ulink url="http://castle-engine.sourceforge.net/view3dscene.php" />.
Then download our demo models from
<ulink url="http://castle-engine.sourceforge.net/demo_models.php" />.
You can now run <literal>view3dscene</literal>,
and open with it various models inside <literal>demo_models/compositing_shaders/</literal>
subdirectory. Also the water demos inside <literal>demo_models/water/</literal>
should be interesting.</para>

<para>Effects may define and use their own uniform variables, including textures,
just like the standard shader nodes. So we can combine any number of textures
inside an effect. As an example we wrote a simple effect that mixes a couple of
textures based on a terrain height (see <xref linkend="figure.terrain" />).
We could also pass any other uniform value to the effect, for example
passing the current time from an X3D <literal>TimeSensor</literal> allows to make
animated effects.</para>

<figure id="figure.terrain">
  <title>ElevationGrid with 3 textures mixed (based on the point height) inside the shader</title>
  <mediaobject>
  <imageobject role="html"> <imagedata format="PNG" fileref="images/terrain.png" /> </imageobject>
  <imageobject role="fo">   <imagedata format="PNG" fileref="images/terrain.png" width="&figure_width;" contentwidth="&figure_width;" /> </imageobject>
  </mediaobject>
</figure>

<para>We can wrap 2D or 3D noise inside a <literal>ShaderTexture</literal>
(see <xref linkend="figure.noise" />).
A texture node like <literal>NoiseTexture</literal> from InstantReality
<xref linkend="bib.instantreality.noisetex" />
may be implemented on GPU by a simple prototype using the <literal>ShaderTexture</literal>.
</para>

<figure id="figure.noise">
  <title>3D and 2D smooth noise on GPU, wrapped in a ShaderTexture</title>
  <mediaobject>
  <imageobject role="html"> <imagedata format="PNG" fileref="images/noise.png" /> </imageobject>
  <imageobject role="fo">   <imagedata format="PNG" fileref="images/noise.png" width="&figure_width;" contentwidth="&figure_width;" /> </imageobject>
  </mediaobject>
</figure>

<para><emphasis>Water</emphasis> is very nice to implement with the help of our effects,
as a proper water simulation
is naturally a combination of a couple effects.
To simulate waves we want to vary vertex
heights, or vary per-fragment normal vectors (for best results,
we want to do both things).
We also want to simulate the fact that water has reflections and
is transparent. We have implemented a nice water using this approach,
with (initially) two independent effect nodes. See <xref linkend="figure.water" />.</para>

<para>We were also able to easily test
two alternative approaches for generating water normal vectors.
One approach was to take normals from the pre-recorded sequence of images
(encoded inside X3D <literal>MovieTexture</literal>,
with noise images generated by the <emphasis>Blender</emphasis> renderer).
The second approach was to calculate normals on the GPU from
a generated smooth 3D noise.
The implementation of these two approaches is contained in two separate
<literal>Effect</literal> nodes, and is concerned
only with calculating the normal vectors in the object space.
Yet another <literal>Effect</literal> node is responsible for
transforming these normal vectors into the eye space.
This way we have extracted all the common logic into a separate effect,
making it clear where the alternative versions differ and what they have
in common.
This was possible because one effect can define
new plug names, that can be used by the other effects.</para>

<para>As for the question <quote>Which approach to generating water normals
turned out to be better?</quote>
Predictably, we showed that using GPU noise is slower, requires a better GPU,
but also improves the quality noticeably. With GPU noise, there is no problem
with aliasing of the noise texture and the noise parameters can be adjusted
in real-time.</para>

<para role="figure_2_column">
<figure id="figure.water">
  <title>Water using our effects framework</title>

<informaltable frame="none">
  <tgroup cols="2" align="center">
  <colspec colnum='1' colname='col1' colwidth='1*'/>
  <colspec colnum='2' colname='col2' colwidth='1*'/>

  <tbody>
    <row><entry>
      <mediaobject>
      <imageobject role="html"> <imagedata format="PNG" fileref="images/water_shaders_0.png" /> </imageobject>
      <imageobject role="fo">   <imagedata format="PNG" fileref="images/water_shaders_0.png" width="&figure_width_2col;" contentwidth="&figure_width_2col;" /> </imageobject>
      </mediaobject>
      Per-pixel lighting.
    </entry>
    <entry>
      <mediaobject>
      <imageobject role="html"> <imagedata format="PNG" fileref="images/water_shaders_1.png" /> </imageobject>
      <imageobject role="fo">   <imagedata format="PNG" fileref="images/water_shaders_1.png" width="&figure_width_2col;" contentwidth="&figure_width_2col;" /> </imageobject>
      </mediaobject>
      Bump mapping. <!-- (with per-pix lighting) -->
    </entry></row>
    <row><entry>
      <mediaobject>
      <imageobject role="html"> <imagedata format="PNG" fileref="images/water_shaders_2.png" /> </imageobject>
      <imageobject role="fo">   <imagedata format="PNG" fileref="images/water_shaders_2.png" width="&figure_width_2col;" contentwidth="&figure_width_2col;" /> </imageobject>
      </mediaobject>
      Reflections and refractions. <!-- (with per-pix lighting) --> <!-- (by a single environment cube map texture).-->
    </entry>
    <entry>
      <mediaobject>
      <imageobject role="html"> <imagedata format="PNG" fileref="images/water_shaders_3.png" /> </imageobject>
      <imageobject role="fo">   <imagedata format="PNG" fileref="images/water_shaders_3.png" width="&figure_width_2col;" contentwidth="&figure_width_2col;" /> </imageobject>
      </mediaobject>
      All effects.
    </entry></row>
  </tbody>
  </tgroup>
</informaltable>
</figure>
</para>

<para>
We also have plugs to change the geometry in object space.
Again, since the effect is integrated with all the browser shaders,
you only need to code a simple function to change the vertex positions
as you want. The effect instantly works with all the lighting and texturing
conditions.
Since the transformation is done on GPU, there's practically
no speed penalty for animating thousands of flowers in our test scene.
See <xref linkend="figure.flowers" />.
</para>

<figure id="figure.flowers">
  <title>Flowers bending under the wind, transformed on GPU in object space</title>
  <mediaobject>
  <imageobject role="html"> <imagedata format="PNG" fileref="images/flowers.png" /> </imageobject>
  <imageobject role="fo">   <imagedata format="PNG" fileref="images/flowers.png" width="&figure_width;" contentwidth="&figure_width;" /> </imageobject>
  </mediaobject>
</figure>

<para>We would like to emphasize that all the effects demonstrated here
are theoretically already possible to implement using the standard
X3D <emphasis>Programmable shaders component</emphasis> <xref linkend="bib.x3d_shaders" />.
However, such implementation
would be extremely cumbersome.
You would first have to implement all the necessary multi-texturing, lighting,
shadows, and other rendering features in a shader code.
This is a large work if we consider all the X3D rendering options.
Also note that a shader should remain optimized for a particular setting.
<!--
Actually, it isn't even possible, unless we can calculate some global options,
like which light sources and fog nodes affect the given shape.
-->
The only manageable way to do this, that would work for all the lighting
and texturing conditions, is to write a shader generator program.
Which is actually exactly what our effects already do for you &mdash;
the implementation of our effects constructs and links
the appropriate shader code, gathering the information from all the nodes
that affect the given shape. The information is nicely integrated
with X3D nodes, effects are specified at suitable nodes, and their
uniform values and attributes are integrated with X3D fields.</para>

</chapter>

<chapter id="chapter.implementation">
<title>Implementation</title>

<para>We have implemented everything described in this paper
in our open-source (LGPL) 3D game engine,
see <xref linkend="bib.castleengine" />.</para>

<para>Our current implementation supports only one shading
language: &glsl;,
the <emphasis>OpenGL Shading Language</emphasis>.
As our engine is cross-platform and focused on OpenGL, this is
the most natural shading language for us.
However, we have designed our extensions
to be applicable to other shading languages (like &cg; and
&hlsl;) as well.<!--
And we believe they can be handled in a very similar fashion.--></para>

<para>One notable concept of &glsl;
is the <emphasis>separate compilation units</emphasis>.
It means that a code can be split into
many units which are parsed and compiled separately,
and only linked together. This allows to write cleaner shader code
(you cannot use undeclared functions from other shader parts).
It also naturally matches with our effects definition,
as each <literal>EffectPart</literal> becomes simply one compilation unit.</para>

<para>A similar feature is found in many other
programming languages, under the names of <quote>units</quote> or <quote>modules</quote>.
But it is not available in two other popular <emphasis>shading</emphasis>
languages: &cg; and &hlsl;.
<!--
and it gives us better line numbers in error messages
(although a pre-processor directive like <literal>#line</literal> could also be used for
this).
-->
To make sure that our idea is applicable to these shading languages,
we have explicitly tested that even without
the <emphasis>separate compilation units</emphasis>,
our implementation still works without problems.
<!--So implementation based on &cg; or &hlsl;
should be definitely possible.-->
<!-- It will not enforce the clean separation
of effect parts &mdash; but that's just a lack of &cg; and &hlsl;.
-->
</para>

<section id="section.pseudo_code">
<title>Outline of the implementation</title>

<para>We present a pseudo-code
that generates a complete shader source for rendering a given shape.
It takes into account standard rendering features (X3D light sources,
textures and such), custom shaders (by X3D nodes like
<literal>ComposedShader</literal>) and our shaders' effects
(by <literal>Effect</literal> nodes). All effects are properly added
together to the final shader code.</para>

<para>We keep the final shader code as three arrays of strings.
Each array keeps code for a specific shader type: geometry, vertex and fragment.
Each string corresponds to a compilation unit for &glsl;,
for other shading languages the strings can be just concatenated at the end.
To make the plugs actually work, we add calls to their functions.
We also add <emphasis>external function declarations</emphasis> at appropriate places.
For languages other than &glsl;, they will simply become <emphasis>forward
function declarations</emphasis> when all the parts are concatenated.</para>

<section id="section.helpers">
<title>Helper functions</title>

<para>First define a function <literal>Plug</literal>.
<!-- It is a low-level, helper function, dealing with strings and string lists. -->
It is responsible for actual text processing that makes the plug functions
correctly called. The argument <literal>PlugValue</literal>
is scanned for all <literal>PLUG_xxx</literal> function definitions.</para>

<para>The argument <literal>CompleteCode</literal> is
searched for the matching <literal>/* PLUG: xxx ... */</literal> comments.
The final shader (for given shape) in <literal>FinalShader</literal>
is also searched for the <literal>/* PLUG: xxx ... */</literal> comments.
In practice, <literal>CompleteCode</literal> given here is either the <literal>FinalShader</literal>
<!--(for shape and group effects) -->
or the shader for a specific texture or light source.</para>

<para>Appropriate calls and forward declarations are inserted to the
<literal>CompleteCode</literal>. In the process, all handled
<literal>PLUG_xxx</literal> functions inside <literal>PlugValue</literal>
are also renamed to unique names, since a single plug may be overridden
by many effects. The modified <literal>PlugValue</literal>
is inserted to the <literal>CompleteCode</literal> as well.
Effectively, the caller can usually <quote>forget</quote>
about the <literal>PlugValue</literal> value afterwards &mdash;
it has been processed, and correctly added to the <literal>CompleteCode</literal>.
</para>

<screen role="pseudocode">&type;
  TShaderType = (geometry, vertex, fragment);
  TShaderSource = array [TShaderType] of a string list;

&var;
  <emphasis>{ shader for the whole shape }</emphasis>
  FinalShader: TShaderSource;

&procedure; Plug(
  EffectPartType: TShaderType;
  PlugValue: string;
  CompleteCode: TShaderSource);

&var;
  PlugName, ProcedureName, PlugForwardDeclaration: string;

  <emphasis>{ Look for /* PLUG: PlugName (...) */ inside
    given CodeForPlugDeclaration.
    Return if any occurrence found. }</emphasis>
  &function; LookForPlugDeclaration(
    CodeForPlugDeclaration: string list): boolean;
  &begin;
    Result &assign; false
    &foreach; S: string &in; CodeForPlugDeclaration &do;
    &begin;
      AnyOccurrencesHere &assign; false
      &while; we can find an occurrence
        of /* PLUG: PlugName (...) */ inside S &do;
      &begin;
        insert into S a call to ProcedureName,
        with parameters specified inside the /* PLUG: PlugName (...) */,
        right before the place where we found /* PLUG: PlugName (...) */

        AnyOccurrencesHere &assign; true
        Result &assign; true
      &end;

      &if; AnyOccurrencesHere &then;
        insert the PlugForwardDeclaration into S,
        at the place of /* PLUG-DECLARATIONS */ inside
        (or at the beginning, if no /* PLUG-DECLARATIONS */)
    &end;
  &end;

&var;
  Code: string list;
&begin;
  Code &assign; CompleteCode[EffectPartType]

  &while; we can find PLUG_xxx inside PlugValue &do;
  &begin;
    PlugName &assign; the plug name we found, the "xxx" inside PLUG_xxx
    PlugDeclaredParameters &assign; parameters declared at PLUG_xxx function

    <emphasis>{ Rename found PLUG_xxx to something unique. }</emphasis>
    ProcedureName &assign; generate new unique procedure name,
    for example take 'plugged_' + some unique integer

    replace inside PlugValue all occurrences of 'PLUG_' + PlugName
    with ProcedureName

    PlugForwardDeclaration &assign; 'void ' + ProcedureName +
    PlugDeclaredParameters + ';' + newline

    AnyOccurrences &assign; LookForPlugDeclaration(Code)

    <emphasis>{ If the plug declaration is not found in Code, then try to find it
      in the final shader. This happens if Code is special for given
      light/texture effect, but PLUG_xxx is not special to the
      light/texture effect (it is applicable to the whole shape as well).
      For example, using PLUG_vertex_object_space inside
      the X3DTextureNode.effects. }</emphasis>
    &if; &not; AnyOccurrences &and;
       Code &lt;&gt; Source[EffectPartType] &then;
      AnyOccurrences &assign; LookForPlugDeclaration(Source[EffectPartType])

    &if; &not; AnyOccurrences &then;
      Warning('Plug name ' + PlugName + ' not declared')
  &end;

  <emphasis>{ regardless if any (and how many) plug points were found,
    always insert PlugValue into Code. This way EffectPart with a library
    of utility functions (no PLUG_xxx inside) also works. }</emphasis>
  Code.Add(PlugValue)
&end;
</screen>

<para>Using the <literal>Plug</literal> function,
we can create the <literal>EnableEffects</literal> function.
It handles the <literal>effects</literal> list, correctly
processing it and adding to the given <literal>CompleteCode</literal>..</para>

<screen role="pseudocode">&procedure; EnableEffects(
  Effects: list of Effect nodes;
  CompleteCode: TShaderSource);
&begin;
  &foreach; Effect &in; Effects &do;
    &if; Effect.enabled &and;
       Effect.language matches renderer shader language &then;
      &foreach; EffectPart &in; Effect.parts &do;
        Plug(EffectPart.type, GetUrl(EffectPart.url), CompleteCode)
&end;
</screen>

</section>

<section id="section.together">
<title>Final algorithm</title>

<para>Finally, using the above functions,
we actually construct the final shader code for given shape.
All the effects (including effects special to lights and textures)
are correctly applied by the algorithm below.</para>

<screen role="pseudocode">FinalShader &assign; new TShaderSource
set FinalShader to basic rendering code
</screen>

<para>At the beginning, <literal>FinalShader</literal> it set to
a simple code that renders 3D object with no lights, no textures and no effects.
The code contains magic <literal>/* PLUG: xxx ... */</literal>
comments, which will allow to enhance it in the following steps.</para>

<para>The real &glsl; vertex and fragment code used by our engine
at this step may be found in our engine sources. See
<ulink url="http://svn.code.sf.net/p/castle-engine/code/trunk/castle_game_engine/src/x3d/opengl/glsl/template.vs" />
for vertex shader and
<ulink url="http://svn.code.sf.net/p/castle-engine/code/trunk/castle_game_engine/src/x3d/opengl/glsl/template.fs" />
for fragment shader.
The geometry shader is initially empty.</para>

<screen role="pseudocode">&if; a complete custom shader code is provided &then;
  FinalShader &assign; custom shader code
</screen>

<para>The X3D file may contain shader code that should replace
the default shaders, following <xref linkend="bib.x3d_shaders" />
specification. For example, a <literal>ComposedShader</literal> node
may be present with a complete &glsl; code.
We use it at this step.</para>

<para>This step trivially allows the <literal>ComposedShader</literal>
code to also contain plug declarations, like <literal>/&#xFEFF;* PLUG: xxx ... */</literal>.
The same plug names as our default names may be used
(like <literal>texture_apply</literal> and so on),
in which case the same user effects will be useful with the custom shader.
This even allows the browser to add some internal effects
(like shadow maps) to the custom shader template.</para>

<para>Alternatively, the <literal>ComposedShader</literal> may have
a completely different approach to rendering. Then it may expose
a completely different set of plug names, reflecting a different
set of parameters to control. <!-- for the authors.--></para>

<screen role="pseudocode">&foreach; Light &in; shape.Lights &do;
  LightShader &assign; new TShaderSource
  <co id="co.init_light_shader" linkends="callout.init_light_shader" /> set LightShader to basic lighting code (optimized for this Light)
  <co id="co.light_effects" linkends="callout.light_effects" /> EnableEffects(Light.Effects, LightShader)
  <co id="co.light_extract" linkends="callout.light_extract" /> LightContribution &assign; LightShader.ExtractFirst
  <co id="co.light_merge" linkends="callout.light_merge" /> Plug(fragment, LightContribution, FinalShader)
  <co id="co.light_finalize" linkends="callout.light_finalize" /> FinalShader &assign; FinalShader + LightShader
</screen>

<para>A little care is needed to correctly add light sources.
Remember that lights may have user-defined effects that should
be applied only to the specific light source. That's why
we temporarily keep the shader code specific to a given light source
in a separate <literal>LightShader</literal> variable.</para>

<calloutlist>
  <callout arearefs="co.init_light_shader" id="callout.init_light_shader">
    <para>The light source contribution will be linked
    with the final shader also using our plugs.
    We initially add to <literal>LightShader</literal>
    a function called <literal>PLUG_add_light_contribution_side</literal>
    that takes care of calculating light contribution, following normal
    X3D light equations. This function should be optimized for the given
    light type (spot, directional, point) and light parameters
    (for example, lights without an attenuation factor or an infinite radius
    or zero specular term may be optimized at this point).
    If we want <emphasis>Phong shading</emphasis>, this function
    should be added as the first string of the <literal>LightShader[fragment]</literal>.
    If we want <emphasis>Gouraud shading</emphasis>, it should go
    to <literal>LightShader[vertex]</literal> instead.</para>

    <para>The actual initial &glsl; light shader code used by our engine at this step
    may be found in our engine sources, see
    <ulink url="http://svn.code.sf.net/p/castle-engine/code/trunk/castle_game_engine/src/x3d/opengl/glsl/template_add_light.glsl" />.</para>

    <!-- Need to add artificial vertical space to make the FO output look Ok.
         By default, calloutlist has no vertical space between items,
         which is bad for us (our calloutlist contains whole paragraphs).
         See "callout list items should honor list.item.spacing variable",
         https://sourceforge.net/tracker/index.php?func=detail&aid=3160341&group_id=21935&atid=373747 -->
    <para role="fo_vertical_space"></para>
  </callout>

  <callout arearefs="co.light_effects" id="callout.light_effects">
    <para>Next we apply <literal>Effect</literal> nodes specific to this light source.
    After this, <literal>LightShader</literal> contains both the initial code
    and user effects code. Doing it this way means that the standard
    light source plugs (like <literal>light_scale</literal>)
    as well as custom plugs (defined in one <literal>Effect</literal> node
    and used by following <literal>Effect</literal> nodes)
    work correctly.</para>

    <para role="fo_vertical_space"></para>
  </callout>

  <callout arearefs="co.light_extract" id="callout.light_extract">
    <para>After applying user effects, we extract (get and delete)
    from <literal>LightShader</literal> our initial code.
    It may be modified now, since calls to user effects are now present inside.</para>

    <para role="fo_vertical_space"></para>
  </callout>

  <callout arearefs="co.light_merge" id="callout.light_merge">
    <para>The extracted <literal>LightContribution</literal>
    must now be connected with the <literal>FinalShader</literal>
    code. This can be done by a simple call to the <literal>Plug</literal>
    function, which will notice the <literal>PLUG_&shy;add_&shy;light_&shy;contribution_&shy;side</literal>
    present inside <literal>LightContribution</literal>.
    After this operation, everything is connected: final shader
    calls <literal>PLUG_&shy;add_&shy;light_&shy;contribution_&shy;side</literal>,
    which in turn calls user effects on the light source.</para>

    <para role="fo_vertical_space"></para>
  </callout>

  <callout arearefs="co.light_finalize" id="callout.light_finalize">
    <para>Finally, add the remaining code to be linked
    together with the <literal>FinalShader</literal>.
    This step simply adds the strings from one list to the other,
    with no processing.
    </para>

    <para role="fo_vertical_space"></para>
  </callout>
</calloutlist>

<screen role="pseudocode">&foreach; Texture &in; shape.Textures &do;
  TextureShader &assign; new TShaderSource
  set TextureShader to basic code (optimized for this Texture)
  EnableEffects(Texture.Effects, TextureShader)
  TextureApplication &assign; TextureShader.ExtractFirst
  Plug(fragment, TextureApplication, FinalShader)
  FinalShader &assign; FinalShader + TextureShader
</screen>

<para>Texture effects require a similar approach as light effects,
to correctly catch all the ways how plugs may be used.</para>

<para>We start by creating a default shader code that applies the texture,
knowing the texture type (2D, 3D, cube map), texture mode (multiply,
add and such) and other properties. It should follow all X3D texturing
and multi-texturing requirements. The code should define a function
named <literal>PLUG_texture_apply</literal> that can be later connected
to the final shader. It should also declare plug named <literal>texture_color</literal>,
that can be used by user effects for this texture.</para>

<para>We deliberately omit here some details,
that in practice may cause the lighting and texture effects
application to differ a little more. One such detail will be explained
in the <xref linkend="section.shadows_from_multiple_lights" />.</para>

<screen role="pseudocode">EnableEffects(appearance node.Effects, FinalShader)

&foreach; group node containing this shape &do;
  EnableEffects(group node.Effects, FinalShader)
</screen>

<para>Effects specific to a given shape,
and effects for all groups containing this shape, are applied.</para>

<para>The effects at this point may override also lights and textures
plugs, like <literal>light_scale</literal>. That's simply because
we have already added all the lighting and texturing shading code
to the <literal>FinalShader</literal>. Overriding <literal>light_scale</literal>
at this point means that we can scale the contribution of every light
by the same function.</para>

<para>At the end, <literal>FinalShader</literal> is just a collection
of strings forming a shading language source code.
For &glsl;, each string is naturally a <quote>separate compilation unit</quote>,
and can be compiled separately.
For other shading languages, the parts may be simply concatenated together
as necessary.</para>

<para>The full, actual source code of this operation is available
in our engine sources, see the unit GLRendererShader.
Source code is on <ulink url="http://svn.code.sf.net/p/castle-engine/code/trunk/castle_game_engine/src/x3d/opengl/glrenderershader.pas" />.
<!--This unit is used by GLRenderer unit.
, in particular by glrenderer_meshrenderer.inc .-->
</para>

<!--
<para>As this is only a pseudo-code, many optimizations are possible.
The largest optimization, that also complicates this code
a little, is to calculate only hash at the first stage,
and construct actual strings only in the second stage.
-->
</section>

</section>

<section id="section.shadows_from_multiple_lights">
<title>Correct shadows from multiple light sources</title>

<para>A texture may be a shadow map projected from a light source.
In our paper <xref linkend="bib.castleengine.shadowmaps" />
we have noted that the shadow should scale only the appropriate
light source contribution. This allows to observe correct shadows
from multiple light sources.
This means that shadow map textures
must be used in a different stage of the calculation than normal
textures.</para>

<para>Our plugs idea allows to do this, in a clean and concise way.
At the place where <literal>TextureShader</literal> is created in the pseudo-code
from the previous section, we treat shadow maps specially.
If the light source corresponding to the shadow map affects our shape
then we do not apply the texture in a usual way. Instead, we call
the <literal>Plug</literal> function to augment the specific light source shader
with a shadow check, like this:</para>

<screen role="pseudocode">&if; Texture is GeneratedShadowMap &and;
   Texture.light affects the shape &then;
  Plug(fragment,
    'uniform sampler2DShadow shadowMap01;
     void PLUG_light_scale(inout float scale, ...)
     {
       scale *= shadow2DProj(shadowMap01, shadowMap01TexCoord).r;
     }', LightShader);
</screen>

<para>The simple call to <literal>shadow2DProj</literal>
may be replaced with a variant
of the <emphasis>Percentage Closer Filtering</emphasis>
(see <xref linkend="bib.pcf" />).</para>

<para>The calculation of light effects has to be complicated a little
to make it work.
In our simple pseudo-code, we added the effects to the <literal>LightShader</literal>
(by <literal>EnableEffects(Light.Effects, LightShader)</literal>)
and then we immediately folded <literal>LightShader</literal> into the
<literal>FinalShader</literal>. Now, we need to remember the
separate <literal>LightShader</literal> for a longer time,
in case it should be augmented with a shadow map.
Alternatively, we could search
for corresponding shadow maps at the moment
when <literal>LightShader</literal> is created.</para>

</section>

<section id="section.pool">
<title>Pool of shaders</title>

<para>Straightforward use of the pseudo-code above means that we
create new shader for each rendered shape. This works correctly,
but is very time and memory consuming for large scenes.
A good implementation should try to reuse the shaders.
This can be achieved by keeping a <emphasis>pool of available shaders</emphasis>.
For each newly created shader, we calculate a hash value (reflecting
the whole shader configuration) and insert this shader into the pool.
When a new shader is needed, we first look for it in the pool
(using the hash value of the desired configuration), and if we find it
&mdash; we reuse it (increasing the reference count). Only if the shader
is not found, we create a new one. This is quite simple to do,
and it provides a perfect sharing of shaders.</para>

<para>The hash value could be calculated based solely on the final string
of the shader. However, this is too slow in our experience &mdash;
as it means that all the string operations have to be performed before
we even know the hash. It's much faster to calculate
the hash value looking at all the parameters that will affect the generated
shader source, including all the participating <literal>Effect</literal> and <literal>ComposedShader</literal>
nodes, as well as the standard X3D lights sources, textures,
fog parameters and so on. Only when we really need to create a new shader,
then we calculate the actual shader source code and compile it.
This means that our pseudo-code gets a little more complicated:
most operations are in fact delayed. For example, at the first stage
we only iterate over the light sources to update the hash value and remember
the light source parameters. Later, if the actual source code is needed,
we actually construct the shader code using the remembered light source
parameters.
</para>

<para>An additional advantage of the <quote>pool of shaders</quote>
appears when a subset of the needed shaders is known in the advance.
For example, imagine an evening outdoor scene, with a storm in the distance.
The lighting cracks the sky occasionaly, making everything temporarily bright.
In technical words, the scene has highly dynamic lighting,
and the shaders for various lighting conditions must be swapped instantly,
to keep the simulation smooth.
In such case, an application may initialize the pool
to contain some shaders with artificial non-zero use count.
This way, some shader configurations
are always kept initialized and ready to be used immediately.</para>

</section>

<section id="section.speed">
<title>Speed</title>

<para>Very nice thing about our effects framework is that
it does not cause any speed loss. Effects code is just
combined into the final shader code, without any transformations that
could make it slower. Our process of <quote>combining</quote> effects is essentially
adding function calls around. Fortunately, a function call has no speed
penalty. Existing shading languages are defined
such that functions can always be inlined (there is no recursion allowed,
and parameter qualifiers have simple interpretation),
and as far as we know they are actually always inlined by existing
shading language compilers.</para>

</section>

<section id="section.inspect_our_shaders">
<title>Inspect shaders generated by our implementation</title>

<para>You can run our <literal>view3dscene</literal> with <literal>--debug-log-shaders</literal> command-line
option. Output will show you the final shader code generated,
and also the OpenGL log after linking the shaders.
Be sure to redirect the output to a file as it may be quite large.
<!-- and you may want to test it first
with a simple scene with one shape &mdash;
This way you can check what our algorithm produces in practice.
This is a simple way to see the complete shaders
learn about our shader rendering internals.-->
This is a useful way to learn about our shader rendering internals.</para>

<para>Another useful option to try in <literal>view3dscene</literal> is to switch to
<emphasis>View
<phrase role="symbol">&rarr;</phrase> Shaders
<phrase role="symbol">&rarr;</phrase> Enable For Everything</emphasis> mode.
This will force shader rendering for all the shapes,
while by default we use shader rendering only for the shapes that
require particular effects (shaders by <literal>ComposedShader</literal>, effects
described in this paper, shadow maps and such).
Forcing shader rendering for everything allows to see
how our shaders implement the whole X3D lighting and texturing model.
It also forces all the lighting calculation to be done per-pixel, resulting
in perfect specular highlights and spot light shapes.</para>
</section>

</chapter>

<chapter id="chapter.conclusion">
<title>Conclusion</title>

<para>We show a new approach for developing effects using the GPU shading languages.
It allows to combine various shader effects with each other
and with application internal shaders.
Our approach is relatively easy
to implement and allows the authors to directly use the existing GPU shading
languages.
We propose a number of extensions to the X3D,
an open standard for 3D data, to make our effects available for 3D
content authors. We have implemented our approach for the &glsl; shading language.
</para>

</chapter>

<chapter id="acknowledgements">
<title>Acknowledgements</title>

<para>
A lot of people helped and encouraged the development of our VRML/X3D engine,
with it's rendering features and extensions. A big <quote>thank you</quote>
goes to all of you!
</para>

</chapter>

<appendix id="appendix.plugs">
<title>Reference of available plugs</title>

<para>Below is a quick reference of plugs available in our
implementation. We have found these plugs to be sufficient for a wide
range of effects, although of course there's always a place for
changes and improvements.
Remember that you can always define your own plugs in your effects
and shader nodes.</para>

<para>Parameter names are shown below merely to document the parameter
meaning. Of course you can change the parameter names when declaring
your own plug function. To some extent you can also change the parameter
qualifiers:</para>

<itemizedlist>
  <listitem><para>If a parameter below is &_inout;, you can change it to &_in;, or
    &_constin; if you don't want to modify the given value.</para></listitem>

  <listitem><para>You can also change the &_inout; parameter to just &_out;, if
    you want to unconditionally overwrite the given value. Although
    this is usually not advised, as it means that you disable previous
    effects working on this parameter. Most of the time, summing or
    multiplying to the previous value is a better choice.</para></listitem>

  <listitem><para>If a parameter below is shown as &_in;, you can add or remove
    the <emphasis>const</emphasis> qualifier as you wish. Using <emphasis>const</emphasis> may allow the
    shader compiler for additional optimizations.</para></listitem>
</itemizedlist>

<!--
We think that the same set of plugs will be useful for them &mdash; but it's just a theory.
An implementation of our effects for other shading languages may
find other opportunities for plugs.
We also believe that most of our plugs (like <quote>do something in object space</quote>,
<quote>do something in eye space</quote>) are generic enough to be usable
with all shading languages.
-->

<!--
We have initially feared that we will need too many plugs, and each specific effect will need a new plug. Fortunately, in practice, it turns out that a relatively small set of generic plugs (like vertex-in-object-space, vertex-in-eye-space an such) performs very well. We have implemented many different final 3D effects using our effects approach (some examples are mentioned in the previous section), and we consider this set of effects as a good basis, suitable for many effects.
-->

<section id="section.vertex_plugs">
<title>Vertex shader plugs</title>

<screen role="plug_declaration">void <emphasis role="bold">PLUG_vertex_object_space_change</emphasis>(
  inout vec4 vertex_object,
  inout vec3 normal_object)
</screen>

<para>You can modify the vertex position and normal vector in object space here.
If you don't need to modify the vertex position,
consider using the <literal>vertex_object_space</literal>
instead, that may result in more optimized shader.</para>

<screen role="plug_declaration">void <emphasis role="bold">PLUG_vertex_object_space</emphasis>(
  const in vec4 vertex_object,
  inout in vec3 normal_object)
</screen>

<para>Process the vertex and normal in object space. You cannot change the vertex
position here, but you can still change the normal vector.</para>

<screen role="plug_declaration">void <emphasis role="bold">PLUG_vertex_eye_space</emphasis>(
  const in vec4 vertex_eye,
  const in vec3 normal_eye)
</screen>

<para>Process the vertex and normal in eye (camera) space.</para>

</section>

<section id="section.fragment_plugs">
<title>Fragment shader plugs</title>

<screen role="plug_declaration">void <emphasis role="bold">PLUG_fragment_eye_space</emphasis>(
  const vec4 vertex_eye,
  inout vec3 normal_eye)
</screen>

<para>Process the vertex and normal in eye space, at the fragment shader.
You can modify the normal vector here, this is useful for bump mapping.</para>

<para>Note that if you modify here normal vector,
you may have to take care to properly negate it. When <literal>gl_FrontFacing</literal>
is false, we're looking at the other side than where standard <literal>gl_Normal</literal>
was pointing. For example, for bump mapping, it's most sensible to negate
only the Z component of the normal vector in tangent space.</para>

<screen role="plug_declaration">void <emphasis role="bold">PLUG_light_scale</emphasis>(
  inout float light_scale,
  const in vec3 normal_eye,
  const in vec3 light_dir,
  const in gl_LightSourceParameters light,
  const in gl_LightProducts light_products,
  const in gl_MaterialParameters material)
</screen>

<para>Scale the given light source contribution.
This plug is also available at light source nodes' effects.</para>

<screen role="plug_declaration">void <emphasis role="bold">PLUG_add_light_contribution_front</emphasis>(
  inout vec4 color,
  const in vec4 vertex_eye,
  const in vec3 normal_eye,
  const in gl_MaterialParameters material)
</screen>

<para>Add pixel color for a lit material. This is typically used to add the light sources.
There is also the <literal>add_light_contribution_back</literal>,
for light contribution on the back side of the faces.</para>

<screen role="plug_declaration">void <emphasis role="bold">PLUG_texture_color</emphasis>(
  inout vec4 texture_color,
  [const in samplerXxx texture,]
  const in vec4 tex_coord)
</screen>

<para>Calculate or modify the texture color.
This plug is available for texture effects. The second parameter
is special: for <literal>ShaderTexture</literal>, it doesn't exist at all.
For other texture nodes, the sampler type depends on the corresponding
X3D texture node: <literal>sampler2D</literal> for 2D textures,
<literal>sampler3D</literal> for 3D textures, <literal>samplerCube</literal> for cube
maps, and <literal>sampler2DShadow</literal> for <literal>GeneratedShadowMap</literal>.</para>

<screen role="plug_declaration">void <emphasis role="bold">PLUG_texture_apply</emphasis>(
  inout vec4 fragment_color,
  const in vec3 normal_eye)
</screen>

<para>At this point, the textures are applied. You can change the fragment
color now, for various effects.</para>

<screen role="plug_declaration">void <emphasis role="bold">PLUG_fog_apply</emphasis>(
  inout vec4 fragment_color,
  const vec3 normal_eye_fragment)
</screen>

<para>At this point, the X3D fog is applied. Again you can change here the
fragment color, as you desire. This plug is called after
the <literal>texture_apply</literal>, because you usually want to apply the fog
to the final (textured) fragment color.</para>

<screen role="plug_declaration">void <emphasis role="bold">PLUG_fragment_end</emphasis>(
  const in vec4 fragment_color)
</screen>

<para>Do the final processing of the fragment. This is called after applying
both textures and fog, and cannot modify the fragment color anymore.
This is useful for operations like alpha-testing the fragment.</para>

<screen role="plug_declaration">void <emphasis role="bold">PLUG_material_light_diffuse</emphasis>(
  inout vec4 diffuse,
  const in vec4 vertex_eye,
  const in vec3 normal_eye,
  const in gl_LightSourceParameters light_source,
  const in gl_MaterialParameters material)
</screen>

<para>Diffuse color at each fragment may be changed here.
This is usually a multiplication of material and light diffuse colors,
but you can change it here into anything you like.</para>

<screen role="plug_declaration">void <emphasis role="bold">PLUG_lighting_apply</emphasis>(
  inout vec4 fragment_color,
  const vec4 vertex_eye,
  const vec3 normal_eye_fragment)
</screen>

<para>At this point, the lighting is calculated. Light contributions are summed,
along with material emissive and global scene ambient colors,
result is clamped to 1.0, and the alpha value is set correctly.
You can change now the fragment color, if you want to do something <emphasis>before</emphasis>
texturing is applied.</para>

</section>
</appendix>

<bibliography id="bibliography">
<title>References</title>

<bibliomixed id="bib.x3d">
  <abbrev>X3D</abbrev>
  <author><othername>Web3D Consortium</othername></author>.
  <title><emphasis>Extensible 3D (X3D) Graphics Standard</emphasis></title>.
  <pubdate>2008</pubdate>.
  <bibliomisc>ISO/IEC 19775-1.2:2008. See <ulink url="http://web3d.org/x3d/specifications/" />.</bibliomisc>
</bibliomixed>

<bibliomixed id="bib.x3d_shaders">
  <abbrev>X3D Shaders</abbrev>
  <bibliomset relation='article'>
    <abbrev>X3D Shaders</abbrev>
    Gonçalo Nuno Moutinho de Carvalho, Tony Gill and Tony Parisi.
    <title role='article'>X3D programmable shaders</title>.
  </bibliomset>
  <bibliomset relation='journal'>
    <title>Proceedings of the ninth international conference on 3D Web technology</title>.
    <publishername>ACM</publishername>, <pubdate>2004</pubdate>.
  </bibliomset>
  <bibliomisc>Available online as part of the X3D specification, see <ulink url="http://web3d.org/x3d/specifications/ISO-IEC-19775-1.2-X3D-AbstractSpecification/Part01/components/shaders.html" /></bibliomisc>
</bibliomixed>

<bibliomixed id="bib.glsl">
  <abbrev>GLSL</abbrev>
  Khronos Group.
  <title><emphasis>OpenGL Shading Language</emphasis></title>.
  <bibliomisc>See <ulink url="http://www.opengl.org/documentation/glsl/" />.</bibliomisc>
</bibliomixed>

<bibliomixed id="bib.glsl_book">
  <abbrev>GLSL Book</abbrev>
  Randi J. Rost.
  <title><emphasis>OpenGL Shading Language</emphasis></title>.
  <publishername>Addison-Wesley</publishername>, <pubdate>2004</pubdate>.
</bibliomixed>

<bibliomixed id="bib.cg">
  <abbrev>Cg</abbrev>
  <title><emphasis>Cg - The Language for High-Performance Realtime Graphics</emphasis></title>.
  NVidia.
  <bibliomisc>See <ulink url="http://developer.nvidia.com/page/cg_main.html" />.</bibliomisc>
</bibliomixed>

<bibliomixed id="bib.hlsl">
  <abbrev>HLSL</abbrev>
  Microsoft.
  <title><emphasis>HLSL</emphasis></title>.
  <bibliomisc>See <ulink url="http://msdn.microsoft.com/en-us/library/bb509561\%28v=vs.85\%29.aspx" />.</bibliomisc>
</bibliomixed>

<bibliomixed id="bib.sh">
  <abbrev>Sh</abbrev>
  Stefanus Du Toit and Michael McCool.
  <title><emphasis>Metaprogramming GPUs with Sh</emphasis></title>.
  <publishername>A K Peters/CRC Press</publishername>, <pubdate>2004</pubdate>.
  <!-- Links to "Metaprogramming GPUs with Sh":
   http://www.amazon.com/Metaprogramming-GPUs-Sh-Stefanus-Toit/dp/1568812299
   http://books.google.com/books?id=8RX4RmFRLmgC&pg=PA79&lpg=PA79&dq=ShAttrib2f&source=bl&ots=IOxd1-BQLL&sig=AIPgFRmbodNpCLWmwk-QV5JUjVo&hl=pl&ei=EylRTe7RB8qeOsiQ1aQI&sa=X&oi=book_result&ct=result&resnum=7&ved=0CEkQ6AEwBg#v=onepage&q=ShAttrib2f&f=false
  -->
</bibliomixed>

<bibliomixed id="bib.ogre_shader">
  <abbrev>OGRE Shader</abbrev>
  <title><emphasis>OGRE Wiki - RT Shader System</emphasis></title>.
  <bibliomisc>See <ulink url="http://www.ogre3d.org/tikiwiki/RT+Shader+System&amp;structure=Development" />.</bibliomisc>
</bibliomixed>

<bibliomixed id="bib.anysl">
  <abbrev>AnySL</abbrev>
  <bibliomset relation='article'>
    Ralf Karrenberg, Dmitri Rubinstein, Philipp Slusallek and Sebastian Hack.
    <title role='article'>AnySL: efficient and portable shading for ray tracing</title>.
  </bibliomset>
  <bibliomset relation='journal'>
    <title>Proceedings of the Conference on High Performance Graphics</title>.
    <publishername>Eurographics Association</publishername>, <pubdate>2010</pubdate>.
  </bibliomset>
  <bibliomisc>See <ulink url="http://portal.acm.org/citation.cfm?id=1921479.1921495" />. See also AnySL website <ulink url="http://www.cdl.uni-saarland.de/projects/anysl/" />.</bibliomisc>
</bibliomixed>

<bibliomixed id="bib.spark">
  <abbrev>Spark</abbrev>
  <bibliomset relation='article'>
    Tim Foley and Pat Hanrahan.
    <title role='article'>Spark: Modular, Composable Shaders for Graphics Hardware</title>.
  </bibliomset>
  <bibliomset relation='journal'>
    <title>Proceedings of SIGGRAPH 2011</title>.
    <publishername>ACM</publishername>, <pubdate>2011</pubdate>.
  </bibliomset>
  <bibliomisc>See <ulink url="http://graphics.stanford.edu/papers/spark/" />.</bibliomisc>
</bibliomixed>

<bibliomixed id="bib.web3d2010.declarativeshader">
  <abbrev>X3D DeclarativeShader</abbrev>
  <bibliomset relation='article'>
    Karsten Schwenk, Yvonne Jung, Johannes Behr and Dieter W. Fellner.
    <title role='article'>A modern declarative surface shader for X3D</title>.
  </bibliomset>
  <bibliomset relation='journal'>
    <title>Proceedings of the 15th International Conference on Web 3D Technology</title>.
    <publishername>ACM</publishername>, <pubdate>2010</pubdate>.
  </bibliomset>
  <bibliomisc>See <ulink url="http://doi.acm.org/10.1145/1836049.1836051" />.</bibliomisc>
</bibliomixed>

<bibliomixed id="bib.humus.volumetricfog">
  <abbrev>Volumetric Fog</abbrev>
  <firstname>Emil</firstname> <surname>Persson</surname> "Humus".
  <title><emphasis>Volumetric Fogging 2</emphasis></title>.
  <pubdate>2006</pubdate>.
  <bibliomisc>See <ulink url="http://www.humus.name/index.php?page=3D&amp;ID=70" />. Nice overview also on <ulink url="http://www.evl.uic.edu/sjames/cs525/shader.html" />.</bibliomisc>
</bibliomixed>

<bibliomixed id="bib.castleengine.shadowmaps">
  <abbrev>X3D Shadow Maps</abbrev>
  <bibliomset relation='article'>
    <firstname>Michalis</firstname> <surname>Kamburelis</surname>.
    <title role='article'>Shadow maps and projective texturing in X3D</title>.
  </bibliomset>
  <bibliomset relation='journal'>
    <title>Proceedings of the 15th International Conference on Web 3D Technology</title>.
    <publishername>ACM</publishername>, <pubdate>2010</pubdate>.
  </bibliomset>
  <bibliomisc>See <ulink url="http://castle-engine.sourceforge.net/x3d_extensions_shadow_maps.php" />.</bibliomisc>
</bibliomixed>

<bibliomixed id="bib.castleengine.bumpmapping">
  <abbrev>X3D Bump Mapping</abbrev>
  <firstname>Michalis</firstname> <surname>Kamburelis</surname>.
  <title><emphasis>Bump mapping extensions</emphasis></title>.
  <pubdate>2008</pubdate>.
  <bibliomisc>See <ulink url="http://castle-engine.sourceforge.net/x3d_extensions.php#section_ext_bump_mapping" /></bibliomisc>
</bibliomixed>

<bibliomixed id="bib.castleengine.texcoordbounds">
  <abbrev>X3D TexCoord Bounds</abbrev>
  <firstname>Michalis</firstname> <surname>Kamburelis</surname>.
  <title><emphasis>Tex coord generation dependent on bounding box</emphasis></title>.
  <pubdate>2010</pubdate>.
  <bibliomisc>See <ulink url="http://castle-engine.sourceforge.net/x3d_extensions.php#section_ext_tex_coord_bounds" />.</bibliomisc>
</bibliomixed>

<bibliomixed id="bib.castleengine.projectivetexturing">
  <abbrev>X3D Projective Texturing</abbrev>
  <firstname>Michalis</firstname> <surname>Kamburelis</surname>.
  <title><emphasis>Projective texture mapping</emphasis></title>.
  <pubdate>2010</pubdate>.
  <bibliomisc>See <ulink url="http://castle-engine.sourceforge.net/x3d_extensions_shadow_maps.php#section_texture_projective" />.</bibliomisc>
</bibliomixed>

<bibliomixed id="bib.instantreality.noisetex">
  <abbrev>NoiseTexture</abbrev>
  <author><othername>Instant Reality</othername></author>.
  <title><emphasis>NoiseTexture</emphasis></title>.
  <bibliomisc>See <ulink url="http://doc.instantreality.org/documentation/nodetype/NoiseTexture/" />.</bibliomisc>
</bibliomixed>

<bibliomixed id="bib.pcf">
  <abbrev>GPU Gems PCF</abbrev>
  <bibliomset relation='article'>
    Michael Bunnell and Fabio Pellacini.
    <title role='article'>Shadow Map Antialiasing</title>.
    Chapter 11 of:
  </bibliomset>
  <bibliomset relation='journal'>
    <title>GPU Gems</title>.
    <pubdate>2004</pubdate>.
  </bibliomset>
  <bibliomisc>Available online on <ulink url="http://http.developer.nvidia.com/GPUGems/gpugems_ch11.html" />.</bibliomisc>
</bibliomixed>

<bibliomixed id="bib.castleengine">
  <abbrev>Castle Game Engine</abbrev>
  <firstname>Michalis</firstname> <surname>Kamburelis</surname>.
  <bibliomisc>The engine webpage, with information and downloads of our tools (like <literal>view3dscene</literal>):
    <ulink url="http://castle-engine.sourceforge.net/" />.</bibliomisc>
</bibliomixed>

</bibliography>

</book>
