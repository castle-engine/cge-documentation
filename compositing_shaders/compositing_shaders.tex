% "Compositing shaders in X3D" by Michalis Kamburelis,
% for Web3D 2011.
%
% If accepted, this will be Â© Copyright 2011 by ACM, Inc.
% See http://www.acm.org/publications/policies/copyright_policy
%

%% Template stuff begins here ------------------------------------------------

\documentclass{acmsiggraph}                     % final
%\documentclass[annualconference]{acmsiggraph}  % final (annual conference)
%\documentclass[review]{acmsiggraph}            % review
%\documentclass[widereview]{acmsiggraph}        % wide-spaced review
%\documentclass[preprint]{acmsiggraph}          % preprint

\usepackage[scaled=.92]{helvet}
\usepackage{times}
\usepackage{graphicx}
\usepackage{parskip}
\usepackage[labelfont=bf,textfont=it]{caption}
\onlineid{1010} %% I think this doesn't matter

%% Template stuff ends here ------------------------------------------------

% This is really the only sensible way to make breaking of monospace text
% (everything inside \texttt, including (but not limited) to urls).
% Otherwise, the monospace text flows outside of the column all over the place.
\sloppy

\usepackage{needspace}

%% For href
\usepackage{ifpdf}
\ifpdf
  \usepackage[pdftex]{hyperref}
\else
  \usepackage[hypertex]{hyperref}
\fi

%% Put float in a nice box,
%% http://en.wikibooks.org/wiki/LaTeX/Floats,_Figures_and_Captions
\usepackage{float}
\floatstyle{boxed}
\newfloat{mycodecore}{H}{listofmycode}
\floatname{mycodecore}{}

% Fix mycodecore: it has additional vertical line at the bottom after the frame.
% Possibly due to Verbatim inside?
\newenvironment{mycode}
{\begin{mycodecore}}
{\end{mycodecore}
\vspace{-0.1in}}

%% Use verbatim that allows \latex commands inside,
%% highly useful for my node spec figures.
%% See http://scott.sherrillmix.com/blog/category/programmer/latex/,
%% http://www.ctan.org/tex-archive/macros/latex/contrib/fancyvrb/
\usepackage{fancyvrb}

%% To allow tex_projected at the bottom.
%% Without this, figure* can only go to the top (or separate page).
%% http://en.wikibooks.org/wiki/LaTeX/Floats,_Figures_and_Captions#Wide_figures_in_two_column_documents
\usepackage{stfloats}

%% Bold inside our code/spec samples.
\newcommand*{\codeem}[1]{\textbf{#1}}

% \nolinkurl is a trick to allow links to break across lines in pdf,
% from http://www.miwie.org/tex-refs/html/latex-packages.html#hyperref
\newcommand*{\myhref}[2]{\texttt{\href{#1}{\nolinkurl{#2}}}}

% The above \myhref unfortunately (due to \nolinkurl trick)
% makes it impossible to break links manually.
% So below is the same as \myhref, but does not break link text automatically,
% and allows to break it manually (by inserting \\ etc.)
% \newcommand*{\myhrefm}[2]{\texttt{\href{#1}{#2}}}

\newenvironment{myenumerate}
{\begin{enumerate}
  \setlength{\itemsep}{0pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}}
{\end{enumerate}}

\title{Compositing shaders in X3D}

\author{Michalis Kamburelis\thanks{e-mail: michalis.kambi@gmail.com}\\Institute of Computer Science\\University of Wroc{\l}aw, Poland}

\keywords{X3D graphics, shaders, GLSL, shadows, shadow maps, bump mapping}

\begin{document}

\teaser{
  \includegraphics[width=2.19in]{rhan_shrine_5_everything}
%%  \caption{Bump mapping and 2 shadow maps on the same shape.}
}

\maketitle

\begin{abstract}
We present a flexible system for implementing effects in X3D
using the shading languages.
Our approach makes the effects naturally integrated with appropriate
X3D nodes, like shape's appearance, light sources and textures.
Our effects seamlessly cooperate with each other and with
the shaders used internally by the X3D browser.
Thus the effects are reusable, and work in various combinations
and under various lighting and texture conditions.
This makes the GPU shaders more useful to the authors.
The browser implementation may also gain from using the same approach
for composing internal shaders.
\end{abstract}

\begin{CRcatlist}
  \CRcat{I.3.7}{Computer Graphics}{Three-Dimensional Graphics and Realism}{Color, shading, shadowing, and texture};
  \CRcat{I.3.6}{Computer Graphics}{Methodology and Techniques}{Languages, Standards}
\end{CRcatlist}

\keywordlist

\section{Introduction}

%% The ``\copyrightspace'' command must be the first command after the
%% start of the first section of the body of your paper. It ensures the
%% copyright space is left at the bottom of the first column on the first
%% page of your paper.
\copyrightspace

X3D \cite{x3d:spec} is an open standard for representing rich 3D data.
Many advanced graphic features are available for the authors.
In particular, we can use \emph{shaders} to change the look of the rendered
geometry.
\emph{Shaders} are programs usually executed on the graphic processor unit
(GPU). They control the per-vertex and per-pixel processing,
for example summing the lights contribution
and mixing the texture colors.

The X3D standard \textit{Programmable shaders component} already makes
the shaders available for the authors. However, the shaders designed using
the standard nodes
\textit{replace} the normal rendering functionality, not \textit{enhance} it.
This reflects the underlying API (like OpenGL or Direct 3D),
and the idea that a shader code should be a complete and optimized program
designed for rendering a particular shape.

We argue that a different approach is needed in many situations.
Authors usually would like to keep the normal rendering features working,
and only add their own effects on top of it. The browser
implementation usually already has an extensive internal shaders system,
and the authors should be able to depend on these internal shaders
to do the common job.

For a simple example, consider the simplified lighting equation with shadows
and bump mapping:

$$ \sum_{l\in Lights} shadow(l) * light\_color(l, material, normal(point)) $$

The \textit{shadow} function returns values in the [0..1] range,
scaling the light color. If the object is not in the shadow, it returns 1.
The \textit{normal} function returns a normal vector at given point.
Now realize that we would like to configure various parts of this equation,
without touching the others.
For example, the \textit{shadow} may check a shadow map pixel,
or (when shadow map is not available) always return 1.
The \textit{normal} function may take the vector straight from
the geometry description, or calculate it using a texture value.
The \textit{light\_color} function may be replaced to use different
light models (Phong, Ward, Cook-Torrance and so on). Sometimes it makes sense
to change these functions only for some chosen light sources, and sometimes
we want to modify all the light sources.

\begin{figure*}[t]
  \centering
  \includegraphics[width=2.3in]{rhan_shrine_0}
  \includegraphics[width=2.3in]{rhan_shrine_1_per_pixel_lighting}
  \includegraphics[width=2.3in]{rhan_shrine_2_bump_mapping}
  \includegraphics[width=2.3in]{rhan_shrine_3_shadow_1st}
  \includegraphics[width=2.3in]{rhan_shrine_4_shadow_2nd}
  \includegraphics[width=2.3in]{rhan_shrine_5_everything}
  \caption{Japanese shrine model with more and more effects applied: Gouraud shading,
Phong shading (per-pixel lighting), bump mapping, shadows from 1st light,
shadows from 2nd light, shadows from both lights.}
%%  \label{fig_xxx}
\end{figure*}

We present a system for creating effects by essentially compositing together
a pieces of shader code. All the effects defined this way effortlessly
cooperate and can be combined with each other and with browser internal
shaders. This makes shader programs:

\begin{enumerate}
\item Much easier to create. We can jump straight into the implementation
  of our imagined algorithm in the shader.
  We are only interested in modifying the relevant shader calculation
  parameter, and we can completely ignore other parts of the shader.

\item Much more powerful. Our effect
  immediately cooperates with absolutely every normal feature of X3D rendering.
  This makes the implemented effect useful for a wide range of real uses,
  not only for a particular situation or a particular model (as it often happens
  with specialized shader code).
  All X3D light sources, textures, even other shader effects,
  are correctly applied.
\end{enumerate}

The important property is that we still keep
the full power of a given GPU shading language.
We deliberately do not try to invent here a new language, or wrap existing
language in some cumbersome limitations. This is most flexible for authors,
and it also allows an easy implementation --- there is no need for any complex
shading language processing inside the X3D browser.

\section{Motivation and previous work}

The popular real-time shading languages (OpenGL \texttt{GLSL}, NVidia \texttt{Cg}, Direct 3D \texttt{HLSL})
don't provide a ready solution for connecting shaders from independent sources.
The \texttt{CgFX} and HLSL \texttt{.fx} files encapsulate shading language code
in \emph{techniques} (for various graphic card capabilities),
and within a single technique specify operations for each rendering pass.
In neither case can we simply connect multiple shader source code files
and expect the result to be a valid program.

An oldest solution to combining effects, used even before the shading languages,
was to perform a multi-pass rendering. Each rendering pass adds or multiplies
the current buffer contents, adding a layer with desired effect.
However, this is expensive --- in each pass we usually have to repeat
some work, like least transforming and clipping the geometry.
It is also not flexible --- you can only modify
the complete result of the previous pass.
%% In our work, we want to allow a single rendering pass to be as
%% powerful as it can.

One approach to writing flexible shader code is to create
a library of functions, and allow the author to choose and compose them
in any order to achieve the desired look. But this approach is very limited,
as it doesn't allow to modify a particular calculation part without
replicating the algorithm outside of this calculation.
For example, if you want to scale the light contribution by a shadow function,
you will have to also replicate the code iterating over the light sources.

Another common solution is to arrange shaders in a pipe, where one
shader processes the result of another. This can be visualized as
layers of materials, where each layer modifies the previous
layer. It is actually similar to a multi-pass rendering approach,
except you don't lose speed on repeating the geometry transformation work.
Still it suffers from the same limitations, as you cannot change
the calculation within an existing layer, without replicating the whole
algorithm of this layer.

%% (see also http://groups.google.com/group/blendertorenderman/browse_thread/thread/aaf07831b91be9db?pli=1 , confirms my findings)

A different approach is presented by the Sh language (\myhref{http://libsh.org/}{http://libsh.org/}, \cite{sh:book}).
It allows writing shaders code (that can run on GPU) directly inside a
C++ program. For this, Sh extends the C++ language (through C++
operator overloading and macros tricks). It allows an excellent
integration between C++ code and shaders, hiding the ugly details of
passing variables between normal code (that executes on CPU) and
shader code (that usually executes on GPU). You can use
object-oriented methods there to create a general shader that can
later be extended through various means, like overriding virtual
methods. However, this is a solution closely coupled with C++. It's
suitable if you have a 3D engine in C++, and you want to use in your
own C++ program and extend its shaders. In this paper, we want to
create a solution that is absolutely separate from the programming
language used to make a browser. Invoking a compiler to generate a
final GPU shader, not to mention teaching user's C++, is out of the
question.

OGRE (\myhref{http://www.ogre3d.org/}{http://www.ogre3d.org/}), an open-source 3D engine written in C++, has a system
for adding shader extensions (see \cite{ogre:shader}). Its idea is similar
to our system (enhance the built-in shaders with your own effects),
however the whole job of combining a shader is done by operating
on particular shader by C++ code. The developer has to code
the logic deciding which shaders are extended, and most of the specification
about how the extension is called is done in the C++ code.
This has the nice advantage of being able to encapsulate some fixed-function
features as well, however the whole system must be carefully controlled by
the C++ code. In our approach, we allow the authors to write direct shading
language code quickly, and the integration is built inside appropriate X3D nodes.

At the end, we would like to mention a solution from a completely
different domain, that is surprisingly similar to ours in some ways.
Drupal (\myhref{http://drupal.org/}{http://drupal.org/}),
an open-source CMS system written in PHP,
has a very nice system of modules. Each module
can extend the functionality of the base system (or other module)
by implementing a \textit{hook}, which is just a normal PHP function
with a special name and appropriate set of parameters. Modules can also define
their own hooks (for use by other modules) and invoke them when appropriate.
This creates a system where it's trivially easy to define a hook,
and to use a hook.
Many modules can implement the same hook and cooperate without any problems.
The whole hook system is defined completely in PHP, as it's a scripting
language, and you can query the list of loaded functions by name,
and call function by its name.

Drupal approach is actually quite similar to our
core idea of combining effects. Our effects are similar to
Drupal's modules, and our ,,plugging points'' are analogous to Drupal hooks.
Our effects can define functions with special names to enhance
the standard shader behavior, just like Drupal modules can define functions
to act on an event from another module.
We can also define new plugs, for other effects to use.
Of course we also have some special problems
(shading language is quite far from a scripting language,
so calling the plugs must be implemented by text replacements inside the browser)
and some special opportunities (we can define effects at
the appropriate nodes of X3D, like textures and lights sources,
as we don't want to throw all the effects in one bag).

% OpenSceneGraph:
% http://www.openscenegraph.org/projects/osg/wiki/Support/Tutorials/ShadersParameters
% http://www.openscenegraph.org/projects/osg/wiki/Support/Tutorials/ShadersIntroduction
% http://mew.cx/osg_glsl_july2005.pdf
% I see no way to connect shaders?
%
% Irrlight --- also no way to connect shaders?
%
% Blender Game Engine: no way to connect shaders, Python code can
% set the final (complete) shader source only?
% http://download.blender.org/documentation/GE/Blender.htm
% http://www.blender.org/development/release-logs/blender-248/realtime-glsl-materials/
% See also source, inside source/gameengine/:
% ./Ketsji/BL_Shader.h
% ./Ketsji/BL_Shader.cpp
% ./Ketsji/BL_BlenderShader.h
% ./Ketsji/BL_BlenderShader.cpp

\section{Plugs: extending the base shader}

The basic idea of our approach is that the base shader code defines
points when a calls to user-defined functions may be inserted. We call
these places \textit{plugs}, as they act like sockets where a logic
may be added. Each plug has a name, and a given set of parameters.
The effects can now use special function names, starting with \texttt{PLUG\_}
and followed by the plug name. These declarations will be found,
and the browser will insert appropriate calls to them from the base shader.

A trivial example of an effect that makes colors two times brighter
follows. This is written in X3D classic encoding,
you should add this inside any \texttt{Appearance} node:

\begin{Verbatim}[commandchars=\\\{\},frame=single]
effects Effect \{
  language "GLSL"
  parts EffectPart \{
    type "FRAGMENT"
    url "data:text/plain,
\textbf{    void PLUG_texture_apply(}
\textbf{      inout vec4 fragment_color,}
\textbf{      const in vec3 normal)}
\textbf{    \{}
\textbf{      fragment_color.rgb *= 2.0;}
\textbf{    \}}"
  \}
\}
\end{Verbatim}

It defines a GLSL function named \texttt{PLUG\_texture\_apply}
within an \texttt{EffectPart} node. The call to this function will
be automatically inserted at the \texttt{texture\_apply} plug point in
the browser internal shader. This particular plug,
the \texttt{PLUG\_texture\_apply}, is called after the normal texture colors
are applied, but before the alpha test, and is a usual place to ,,just modify the pixel color''.
\texttt{fragment\_color} is an \texttt{inout} parameter, by modifying it
you modify the color that will be displayed on screen.

We have a short reference (at the end of this document) of all
the plugging points available in our shaders. For each plugging point,
like this \texttt{PLUG\_texture\_apply}, we define a list of parameters
(you have to declare them exactly the same in your code), and we define
when it is called.

Many usage scenarios are available:

\begin{myenumerate}
\itemsep 0pt
\item Browser internal shaders can be connected with each other. For example,
we have a basic shader code that defines lighting and texturing. Our bump
mapping is implemented by simply plugging the code to modify the normals
into the base shader. This way, the base shader is completely
independent from the bump mapping implementation. We can change the bump
mapping implementations (for example, use bump texture from image or
from a generated random noise) without trouble.

So even inside the browser, this allows for much easier shader handling.

\item User shaders may plug into browser shaders. This is the most usual
case. Our shaders define various points when you can plug your code,
and override or enhance our shading.

\item User shaders can trivially (by just adding a "magic" comment)
define new plugging points, which are available for the following
user shaders on the same shape. This is available for user shaders
defined inside our \texttt{Effect} nodes, and also for the complete user shaders
in standard \texttt{ComposedShader} and similar nodes.

This way some of your effects may be further customized by the other effects.
\end{myenumerate}

When you don't use an explicit shader (like \texttt{ComposedShader} node),
browser uses its own internal basic shader. This shader is then
extended by effects like bump mapping (coming from internal browser
implementation), or by user effects. But you can also define your own
\texttt{ComposedShader} node. Then your own shader is enhanced by the
browser. If you define the same (or compatible) plugging
points, then the browser effects will be even added to your own
shader. And of course user effects are added to your shader.
So the system is useful regardless if you use or not an explicit
shader on the \texttt{Appearance.shaders} list.

\subsection{Effect node}

We define new \texttt{Effect} node that holds information about
the source code and uniform values specific to given effect.

\begin{mycode}
\underline{Effect : X3DChildNode}
\begin{Verbatim}[commandchars=\\\{\}]
SFString [] \codeem{language} ""
  # Just like ComposedShader.language.
  # This effect will be used only when
  # the base shader (browser internal
  # or selected from Appearance.shaders)
  # has the same language.
SFBool [in,out] \codeem{enabled} TRUE
  # Allows to easily turn on/off the effect.
  # You could also remove/add the node
  # from the scene, but often toggling
  # this field is easier for scripts.
MFNode [] \codeem{parts} [] # EffectPart

... you can declare additional
  new fields and events, that will be passed
  as uniform values to the shader,
  just like for ComposedShader node ...
\end{Verbatim}
\end{mycode}

\needspace{1in}
The effect source code is split into a number of parts:

\begin{mycode}
\underline{EffectPart : X3DNode, X3DUrlObject}
\begin{Verbatim}[commandchars=\\\{\}]
SFString [] \codeem{type} "VERTEX"
  # Like ShaderPart.type:
  # allowed values are
  # FRAGMENT | VERTEX | GEOMETRY.
MFString [] \codeem{url} []
  # The source code, like ShaderPart.url.
\end{Verbatim}
\end{mycode}

  %% # May come from an external file (url),
  %% # or inline (following "data:text/plain,").
  %% # In XML encoding, may also be inlined in CDATA.

The functions that enhance standard shaders behavior are recognized
by names starting with \texttt{PLUG\_} in the source code.
You can also freely define your own utility functions.
You can pass uniform variables to the shader,
and you can pass varying variables between the vertex and fragment
shaders, just like with standard shader nodes.

In a single \texttt{EffectPart} node, you can define many \texttt{PLUG\_}
functions. However, you can only plug functions into the declared shader
type. For example, you cannot use the \texttt{texture\_apply} plug within
a \texttt{VERTEX} shader.
If your effect requires some processing per-vertex and some per-fragment,
you will probably use two \texttt{EffectPart} nodes, with appropriate types.
While this may seem like an arbitrary limitation,
this reflects how shader parts are declared in shading languages with
separate namespaces for vertex and fragment parts.
A single part may declare many variables and functions,
but it must be completely contained within a given shader type.

Note that it is completely reasonable to have an \texttt{EffectPart} node
with source code that doesn't define any \texttt{PLUG\_xxx} functions.
Such \texttt{EffectPart} node may be useful for defining shading language
utility functions, used by your other effects.

For shading languages that have separate compilation units
(like the \emph{OpenGL Shading Language}) the implementation may choose to place
each effect part in such separate unit. This forces the shader code to be
cleaner, as you cannot use undeclared functions and uniform variables.
It also allows for cleaner error detection (parsing errors will be detected
inside the given unit).

At one point we tried the approach
to not look at any special function names in shader code,
and instead define a plug name in the separate field of the \texttt{EffectPart}
node. However, the browser may need to know the full declaration
of a function, to make a forward or external declaration of it.
One way to overcome this problem was to force repeating this declaration
in another field. Another way was to split shader code into much more
parts (for example, one part declares the uniform variables, one part declares
the plug function, one part declared the function body and so on).
Both approaches seemed not natural and uncomfortable for authors,
and they didn't really offer much simpler implementation, so we dropped this idea.
The current approach, to find the declarations of \texttt{PLUG\_xxx} functions
inside a complete shader code, is easy to implement, and results in clean
shader code of the effects. It also allows us to naturally use
the separate compilation units in case of GLSL.

\subsection{Effects for particular shapes appearance}

There are various places where an \texttt{Effect} node may be used.
If it's specific to a given shape appearance, you can place it
on the \texttt{Appearance.effects} list:

\begin{mycode}
\underline{Appearance}
\begin{Verbatim}[commandchars=\\\{\}]
MFNode [] \codeem{effects} [] # Effect
\end{Verbatim}
\end{mycode}

All the effects on this list (with suitable language) will be used.
Note that this is different
than the \texttt{Appearance.shaders}, which chooses only one shader.
For effects, we choose all of them.

The most basic idea of this paper is to allow you to define two
independent shader effects, and then seamlessly connect them by simply
placing them both on \texttt{Appearance.effects} list. This also allows to
define a library of effects, that can be composited without any work
needed by user.

\begin{figure}[H]
  \centering
  \includegraphics[width=3in]{fresnel_and_toon}
  \caption{Toon and Fresnel effects combined.}
\end{figure}

Note that all our nodes benefit from X3D mechanism to reuse the nodes
by reference (the \texttt{DEF} / \texttt{USE} keywords). Reusing the
\texttt{Effect} nodes
is most natural, and allows to combine existing effects in any desired way.
Reusing the \texttt{EffectPart} nodes is also useful, when some effects
would like to share a particular piece of code. For example,
you can create an \texttt{EffectPart} with a library of useful
shading language functions, and reuse if for various effects.

\subsection{Effects for a group of nodes}

Our \texttt{Effect} node is a descendant of the abstract \texttt{X3DChildNode}.
As such it can be placed directly within X3D grouping nodes like
\texttt{Group}, \texttt{Transform} and at the top level of the X3D file.
Such effect will apply to all the shapes within given grouping node.
The scope rules follow the X3D conventions for other nodes,
like pointing device sensor nodes and \texttt{LocalFog}.

The \texttt{LocalFog} example is worth emphasizing. Using our system,
a browser can implement the \texttt{LocalFog} node as a prototype
that expands to our \texttt{Effect} node. This gives you a 100\% correct
implementation of the standard \texttt{LocalFog} node, as is trivially easy.

As one of the demos, we have implemented a realistic
animated volumetric fog, where the fog density is stored in
a 3D smooth noise texture (idea from \cite{humus:volumetricfog}).
In a fragment shader, the 3D texture is sampled
along the line between the camera and pixel position. This makes a very
convincing effect of a dense fog. Since it is implemented as an effect,
it can be instantly used with various lighting and texturing conditions
--- it simply works for all X3D shapes.

\begin{figure*}[t]
  \centering
  \includegraphics[width=2.3in]{volumetric_animated_fog_no_fog}
  \includegraphics[width=2.3in]{volumetric_animated_fog_no_light}
  \includegraphics[width=2.3in]{volumetric_animated_fog_all}
  \caption{Volumetric fog scene: 1) No fog; 2) No lighting; 3) Lights and fog.
Note that the fog is assumed to have its own ambient lighting,
so it colors the image even in the 2) case.}
\end{figure*}

\subsection{Light sources effects}

The real power of our system comes from the fact that some nodes,
like light sources and textures, can define their own effects.
So you can easily define a shader specific for a given light source,
or texture.

\needspace{1in}
We add the \texttt{effects} field for every light node:

\begin{mycode}
\underline{X3DLightNode}
\begin{Verbatim}[commandchars=\\\{\}]
MFNode [] \codeem{effects} [] # Effect
\end{Verbatim}
\end{mycode}

This allows you to modify the light source contribution of this particular
light node. For example you can modify the spot light shape, possibly
based on some texture information.
Or you can implement a different lighting model, like anisotropic Ward
or Cook-Torrance.

\begin{figure}[H]
  \centering
  \includegraphics[width=3in]{fancy_light_spot_shape}
  \caption{Textured spot light with shadow.}
\end{figure}

%% , or even replace the default
%% calculation of the light source contribution. In the second case,
%% the default calculation will not even be used in the shader,
%% so it will not slow down the calculation without a reason.

\subsection{Texture effects}

Just like the light sources, also each texture node may define its own effects:

\begin{mycode}
\underline{X3DTextureNode}
\begin{Verbatim}[commandchars=\\\{\}]
MFNode [] \codeem{effects} [] # Effect
\end{Verbatim}
\end{mycode}

You can use the \texttt{effects} field
inside any \texttt{X3DTextureNode} to enhance and modify the look of any
standard texture node, like \texttt{ImageTexture}.
You can use a plug \texttt{texture\_color} to change the texture color,
knowing the current texture coordinates and other information.

\subsubsection{ShaderTexture effects}

We introduce a new X3D node designed specifically for generating
textures using the shaders. This is suitable
if your texture is defined completely using the shading language.
The texture contents are not stored anywhere (not even on GPU),
and the X3D browser doesn't manage any texture resources.
From a GPU point of view, there is no texture\footnote{But the spoon is real,
we swear.}. There is only a shader function that generates colors
based on some vectors. By wrapping such function inside
the new \texttt{ShaderTexture} node, you can treat it much like other X3D textures.
In particular, you can provide texture coordinates (explicit or generated)
for the texture.
Effectively, it behaves like a normal texture node, with all the related
X3D features.

%% The new texture node specification:

\begin{mycode}
\underline{ShaderTexture : X3DTextureNode}
\begin{Verbatim}[commandchars=\\\{\}]
MFNode [] \codeem{effects} [] # Effect
SFString [] \codeem{defaultTexCoord} "BOUNDS2D"
  # ["BOUNDS2D"|"BOUNDS3D"]
\end{Verbatim}
\end{mycode}

Actually, the \texttt{effects} field is already defined at
the \texttt{X3DTextureNode} class (see above). We just mention it here
for completeness.

You should include an effect overriding at least the \texttt{texture\_color}
plug, otherwise texture contents are undefined. Our implementation actually
sets the default texture color to pink (RGB(1,~0,~1)), so it stands out,
reminding you to override it.

The \texttt{defaultTexCoord} field is used only when a particular geometry
doesn't specify the texture coordinates to use for this texture:

\begin{myenumerate}

\item
  \texttt{"BOUNDS2D"} means that the algorithm described in the specification
  for {IndexedFaceSet} should be used. This adapts 2D texture coordinates
  to the two largest bounding box sizes.
  It's most suitable for 2D textures that use only 2D coordinates
  (ignoring other coordinates, or assuming they are always 0,~1).

\item
  \texttt{"BOUNDS3D"} means that the algorithm described in the specification
  of \textit{Texturing3D} component (section \textit{"Texture coordinate generation for primitive objects"})
  is used.
  This adapts 3D texture coordinates to the bounding box sizes.
  It's most suitable for 3D textures (you can ignore the 4th texture coordinate
  component, or treat it as homogeneous, as \texttt{"BOUNDS3D"} will always
  set it to 1).

\end{myenumerate}

Remember that the texture coordinates can be always explicitly specified
at the geometry, by any of the \texttt{X3DTextureCoordinateNode} nodes.
%% Both explicit texture coordinate lists (\texttt{TextureCoordinate}
%% \texttt{TextureCoordinate3D}, \texttt{TextureCoordinate4D})
%% and the coordinate generator nodes
%% (like \texttt{TextureCoordinateGenerator}) are suitable.
%% %% , \texttt{ProjectedTextureCoordinate}
%% Note that our engine has some extensions to allow
%% the \texttt{"BOUNDS2D"} and \texttt{"BOUNDS3D"} values to be also
%% used for the \texttt{TextureCoordinateGenerator.mode} field, see
%% \cite{vrmleng:texcoordbounds}.
The \texttt{defaultTexCoord} says only what is the default approach,
when you don't use \texttt{texCoord} at the geometry node.
The idea is that using a \texttt{ShaderTexture} should be as comfortable
as any other texture node.

%% Projective texture mapping by \texttt{ProjectedTextureCoordinate}
%% is also our extension, see \cite{vrmleng:projectivetexturing}.

\subsubsection{When to use the ShaderTexture}

For textures other than the \texttt{ShaderTexture},
when the \texttt{texture\_color} plugs are called,
the internal shaders have already calculated the initial texture
color by actually sampling the texture image. This is useful if you
want to modify this color. If you'd rather ignore the normal
sampled color, and always override it with your own, consider using
the special \texttt{ShaderTexture} node instead. Using
a normal texture node (like \texttt{ImageTexture}) for this
would be uncomfortable, as you would have to load a dummy texture image,
and the shaders could (depending on optimization) waste some time
on calculating a color that will be actually ignored later.

Remember that in all cases (effects at \texttt{ImageTexture},
at \texttt{ShaderTexture}, etc.) you can always use additional
textures inside the effect. Just like inside a standard \texttt{ComposedShader},
you can declare an \texttt{SFNode} field inside an \texttt{Effect}
to pass any texture node to the shader as uniform value.
This allows you to base your effects calculation on any number of textures,
and combine them in any way you like. The only difference
between \texttt{ShaderTexture} and other textures is what the browser
does automatically for you, that is what color is passed
to the first \texttt{texture\_color} plug.

\begin{figure}[H]
  \centering
  \includegraphics[width=3in]{shader_texture_edge_detection}
  \caption{ShaderTexture doing an edge detection operation on a normal ImageTexture.}
\end{figure}

\subsubsection{Independence from texture filtering}

Note that the shader effects for textures are calculated at each screen fragment
(not at each texel). So your effects are not concerned about the texture size
or texture filtering options. You just use the interpolated texture
coordinates in the \texttt{texture\_color} plug.

\begin{figure}[H]
  \centering
  \includegraphics[width=3in]{shader_texture_no_filtering_problems}
  \caption{Yellowish arc is done by a texture effect, as so is not
    affected by the pixelated look of the base image texture.}
\end{figure}

\section{Defining your own plug points}

In your shader code, you can define your own plug points by a
magic comment:

\begin{mycode}
\begin{Verbatim}[commandchars=\\\{\}]
/* PLUG: name (param1, param2, ...) */
\end{Verbatim}
\end{mycode}

This defines a point when calls to user functions declared as
\texttt{PLUG\_name} will be added. They will be called with given
parameters.

Many effects may use the same \texttt{PLUG\_name},
you can even use the same \texttt{PLUG\_name} many times within a single
effect. All the \texttt{PLUG\_name} functions
will be uniquely renamed to not collide with each other.

The calls will be added in the order they are specified on the
\texttt{effects} list. More precisely, the most local effects
(at light sources and textures) are called first, then the effects
at appearance, and finally the effects inside the grouping nodes.
Although, preferably, for most effects this order will not matter.

A plug is often defined to allow modifying some parameter
repeatedly (like adding or modulating the fragment color),
so one or more of the parameters are often allowed to be processed
as \texttt{inout} parameters.

The same plug name may be declared many times in the source shader.
This means that a single \texttt{PLUG\_xxx} function will be called
many times. This is typically useful when your shader calculation is naturally
expressed as a loop, but you had to unroll this loop for shader source
(for example, to slightly tweak some loop iterations).
The plug names that are available per-light source and per-texture
are an example of this.
%% If you use the \texttt{PLUG\_texture\_color}
%% inside \texttt{Appearance.effects}, you change the color of all
%% the textures (even shader textures).
%% Not really available, as params to texture\_color differ.
If you use the \texttt{PLUG\_light\_scale}
inside \texttt{Appearance.effects}, you change the intensity
of all the light sources on the given shape. Contrast this with using
the same \texttt{PLUG\_light\_scale} inside a \texttt{X3DLightNode.effects},
where you only change the given light node contribution.

Currently all the plugs must be procedures, that is their result type
must be declared as \texttt{void}. We have been considering
a possibility of functions, where part of the calculation may be replaced
by a call to a plugged function. While not difficult to implement,
this idea seems unnecessary after many tests.
Procedural plugs are easier to declare, as the call to the plug
may be simply inserted, while in case of function it will have to replace
some previous code. This also means that using a procedural plug
\textit{never} replaces or removes some existing code, which is a very nice
concept to keep. We want the effects to cooperate with each other,
not to ,,hijack'' from each other some parts of the functionality.

The nice feature of our magic \texttt{/* PLUG ... */} comments is that a shader source
is still valid even if you completely ignore the plugs. For example,
you can write a custom \texttt{ComposedShader} node, defining some plugs,
and for browsers that understand them --- the plugs can be used,
for other browsers --- plugs will be gracefully ignored (but still
the shader will run, although without any effects).

You can define new plug points in your own effects code, as well as in your
complete custom shaders (like \texttt{ComposedShader}) code.
In the latter case, the plug points
are only available for the following effects of the same node.

\subsection{Where the forward declarations are placed}

Special comment \texttt{/* PLUG-DECLARATIONS */} may be used
near the beginning of your shader source code. It is only useful
if your shader code defines any new plugging points, that is if you
have any magic \texttt{/* PLUG: ... */} comments inside.
When some other effect uses your plugging point, the browser adds
an appropriate call to a function in that effect.
Additionally, the browser has to declare the function,
because it may be in a separate compilation unit (in case of GLSL),
or just defined later in the code.
These (forward or external) declarations are inserted at
the point of special \texttt{/* PLUG-DECLARATIONS */}
comment, or (when it is missing) simply at the beginning of your shader source.
This applies in the same way to shader code inside an \texttt{EffectPart},
or inside standard X3D node like \texttt{ShaderPart}.

Using \texttt{/* PLUG-DECLARATIONS */} may be necessary
as some shading language directives are required to be placed before
all normal declarations. For example, in case of OpenGL shading language,
the \texttt{\#version} as well as some \texttt{\#extension} directives
must occur at the beginning of the shader code. You should place
\texttt{/* PLUG-DECLARATIONS */} after such directives,
and before any \texttt{/* PLUG: ... */} declarations.

\subsection{Invalid shader code}

We guarantee the behavior only if the provided shading language code
is a correct, self-contained code.
X3D browser doesn't validate code in any way, so any error (like undeclared
variable, like unterminated block (,,{'' without matching ,,}''),
like unterminated comment) may be only detected after the complete shader
is determined and compiled by the GPU.

Although in case of shading languages with separate compilation units,
the separation is actually better, and parsing errors will cause
problems in your own code only. Still, by writing incorrect code,
you can cause the whole shader to malfunction.

In all our practical cases, this didn't cause any problems.
Since you code each effect separately, you also test them separately,
and in practice it's usually obvious what problem causes the parsing error.
This is also a direct consequence of our decision to \textbf{never require
the browser to parse the shader code}.

It should be noted however that in particularly nasty cases,
a deliberately poorly coded effect may cause troubles for other effects.
In particular, since you can use \#define and macros in your effect code,
you can do nasty tricks to break other effects. You can make them compile,
but function incorrectly. However, we don't consider
it a real problem. You really have to deliberately want to do something bad,
and be familiar with internals about how the shader is generated,
to achieve some particular weird behavior.
It doesn't happen by accident in our experience.
%% And you may need to use the internal knowledge
%% how the other effects are implemented (maybe how they are implemented
%% inside the browser).

Note that this isn't a security problem --- bad shader code only breaks
rendering of a particular shape. And X3D \texttt{ComposedShader} node allows users
to execute any shading language code anyway. So if there's anything dangerous
(for example a buggy OpenGL may cause the browser process to exit with
segmentation fault on some special shader code snippets),
you could do it without using our effects framework anyway.

\section{Various examples}

Remember that effects may define their own uniform variables,
just like the standard shader nodes. So you can pass your own textures
to effects. For example you can write an effect that mixes a couple of textures,
using any information available in the shader as a criteria for mixing.
You can also pass any other values for an effect, for example you can
pass the current time from a \texttt{TimeSensor} and make dynamic effects.

\begin{figure}[H]
  \centering
  \includegraphics[width=3in]{terrain}
  \caption{ElevationGrid with 3 textures mixed inside the shader,
based on the point height.}
\end{figure}

Even inside the browse implementation, using plugs to implement internal
effects allows for a great code simplification and separation.
Many internal effects of our engine, like fog, shadow maps
and the bump mapping have been reimplemented using our plugs approach.
This made their implementation very clean and short,
and nicely separated from each other. It also shows that
X3D authors have the power to implement such effects easily by themselves.

%% without any traditional hassles. For example, bump mapping can be
%% achieved using standard \texttt{ComposedShader} as well,
%% but then you have to write all this ,,boilerplate'' code to also deal
%% with all the possible lighting and textures configurations.
%% Well, the browser is still useful to calculate nice tangent vectors,
%% although an authoring program could generate GLSL attributes for them as well.

%% Same thing with shadow maps, they only plug to the \texttt{light\_scale}
%% calculation of appropriate light.

Another example is \emph{water}. Water is very nice to implement by our effects,
as a proper water simulation
is naturally a combination of a couple effects.
You want to simulate waves, so you want to vary vertex
heights, or vary per-fragment normal vectors (for best results,
you want to do both things).
You also want to simulate the fact that water has reflections, and
is transparent. We have implemented a nice water using this approach,
with two independent effects.

Our approach also allowed us to easily implement and test
two alternative versions for generating water normals.
One approach was to take normals from the pre-recorded sequence of images
(encoded inside \texttt{MovieTexture},
with noise images generated by \emph{Blender} renderer).
The second approach was to calculate them on the GPU from
a generated smooth 3D noise. Thanks to our effects system,
we could immediately test our alternative normal vector approaches,
without touching the water reflection / refraction effect.
We have actually implemented the two alternative effects by overriding
the plug ,,getting the normal vector in object space''.
This plug is defined in another effect, that calls it and transforms
the normal vector into the eye space. This way we have extracted
as much as possible common logic into one effect, making it clear
where the alternative versions differ and what they have in common.
This was possible because one effect can define
new plug names, that can be used by the other effects.

%% (As for the results about which one is better: predictably, we showed
%% that using GPU noise is slower, requires a better GPU,
%% but also improves the quality noticeably. With GPU noise, there is no problem
%% with aliasing of the noise texture, and the noise parameters can be adjusted
%% in real-time.)

\begin{figure*}[t]
  \centering
%  \includegraphics[width=2.3in]{water_shaders_0}
  \includegraphics[width=2.3in]{water_shaders_1}
  \includegraphics[width=2.3in]{water_shaders_2}
  \includegraphics[width=2.3in]{water_shaders_3}
  \caption{Water using our effects: 1) Per-pixel lighting and bump mapping.
2) Per-pixel lighting and reflections and refractions (by a single environment cube map texture).
3) All effects.}
\end{figure*}

We can also wrap 2D and 3D noise inside a \texttt{ShaderTexture}.
A texture node like \texttt{NoiseTexture} from InstantReality
\myhref{http://doc.instantreality.org/documentation/nodetype/NoiseTexture/}{http://doc.instantreality.org/documentation/nodetype/NoiseTexture/}
may be implemented on GPU by a simple prototype using \texttt{ShaderTexture}.

\begin{figure}[H]
  \centering
  \includegraphics[width=3in]{noise}
  \caption{3D and 2D smooth noise on GPU, wrapped in ShaderTexture.}
\end{figure}

We also provide plugs to easily change the geometry in object space.
Again, since the effect is integrated with all the browser shaders,
you only need to code a simple function to change the vertex positions
as you want. The effect instantly works with all the lighting and texturing
conditions. Since the transformation is done on GPU, there's practically
no speed penalty for animating thousands of flowers in our test scene.

\begin{figure}[H]
  \centering
  \includegraphics[width=3in]{flowers}
  \caption{Flowers bending under the wind, transformed on GPU in object space.}
\end{figure}

We would like to emphasize that all the effects demonstrated here
are theoretically already possible to implement by the standard
X3D \textit{Programmable shaders component}. However, such implementation
would be extremely cumbersome.
You would first have to prepare a shader code to calculate all
the (multi-)texturing, lighting,
shadows, and all the other effects that you use in your scene.
This is a large work if you consider all the X3D options, and note that
a shader should remain optimized for a particular setting.
Actually, it isn't even possible, unless you can calculate some global options,
like which light sources and fog nodes affect the given shape.
The only manageable way to do this, that would work for all the lighting
and texturing conditions, is to write a shader generator program.
Which is actually exactly what our effects already do for you ---
the implementation of our effects constructs and links
the appropriate shader code, gathering the information from all the nodes
that affect the given shape. And all the information is nicely integrated
with X3D nodes, effects are specified at suitable nodes, and their
uniform values and attributes are integrated with X3D fields.

\section{Implementation notes: GLSL}

Our implementation is concerned only with the OpenGL Shading Language
(GLSL). We have designed our extensions to be useful with other
shading languages (like Cg or HLSL), and we think that the same set of
plugs will be useful for them --- but it's just a theory. An
implementation of our effects for other shading languages may likely
find other opportunities for plugs.

What we know for a fact is that the GLSL ,,separate compilation
units'' idea is \textit{not} necessary for a successful
implementation of our effects. That's because our initial
implementation for GLSL didn't actually need the separate compilation
of GLSL. ,,Separate compilation units'' of GLSL are useful to force
cleaner shader code (you cannot use undeclared functions from other
shader parts) and it gives you better line numbers in error messages
(although a pre-processor directive like \#line could also be used for
this). But they are not a necessary feature.

\section{Short reference of available plugs}

The table below is a quick reference of plugs available in our
implementation. We have found these plugs to be sufficient for a wide
range of effects, although of course there's always a place for
changes and improvements. See our engine website,
in particular the online reference of our plugs on
\myhref{http://vrmlengine.sourceforge.net/compositing\_shaders.php}{http://vrmlengine.sourceforge.net/compositing_shaders.php},
for the most up-to-date list.
% Remember that you can always define your own plugs in your effects
% and shader nodes.

Parameter names are shown merely to easily document the parameter
meaning. Of course you can change the parameter names when declaring
your own plug function.

You can also change to some extent the parameter qualifiers:

\begin{itemize}
  \item If a parameter below is "inout", you can change it to "in", or
    "const in" if you don't want to modify the given value.
  \item You can also change the "inout" parameter to just "out", if
    you want to unconditionally overwrite the given value. Although
    this is usually not advised, as it means that you disable previous
    effects working on this parameter. Most of the time, summing or
    multiplying to the previous value is a better choice.
  \item If a parameter below is shown as "in", you can add or remove
    the "const" qualifier as you wish. Using "const" may allow the
    shader compiler for additional optimizations.
\end{itemize}

\subsection{Vertex shaders plugs}

\begin{Verbatim}[commandchars=\\\{\},frame=single]
void PLUG_vertex_object_space_change(
  inout vec4 vertex_object,
  inout vec3 normal_object)
\end{Verbatim}

You can modify the vertex and normal vectors in object space here.
If you don't want to modify them, consider using \texttt{vertex\_object\_space}
instead, that may result in more optimized shader.

\begin{Verbatim}[commandchars=\\\{\},frame=single]
void PLUG_vertex_object_space(
  const in vec4 vertex_object,
  const in vec3 normal_object)
\end{Verbatim}

Process the vertex and normal in object space.

\begin{Verbatim}[commandchars=\\\{\},frame=single]
void PLUG_vertex_eye_space(
  const in vec4 vertex_eye,
  const in vec3 normal_eye)
\end{Verbatim}

Process the vertex and normal in eye (camera) space.

\subsection{Fragment shaders plugs}

\begin{Verbatim}[commandchars=\\\{\},frame=single]
void PLUG_fragment_eye_space(
  const vec4 vertex_eye,
  inout vec3 normal_eye)
\end{Verbatim}

Process the vertex and normal in eye space, at fragment shader.
You can modify the normal vector here, this is useful for bump mapping.

\begin{Verbatim}[commandchars=\\\{\},frame=single]
void PLUG_light_scale(
  inout float light_scale,
  const in vec3 normal_eye,
  const in vec3 light_dir,
  const in gl_LightSourceParameters light_source,
  const in gl_LightProducts light_products,
  const in gl_MaterialParameters material)
\end{Verbatim}

Scale the given light source contribution.
This plug is also available at light source nodes' effects.

\begin{Verbatim}[commandchars=\\\{\},frame=single]
void PLUG_add_light_contribution_front(
  inout vec4 color,
  const in vec4 vertex_eye,
  const in vec3 normal_eye,
  const in gl_MaterialParameters material)
\end{Verbatim}

Add pixel color for a lit material. This is typically used for each light source.
There is also the \texttt{add\_light\_contribution\_back},
for light contribution on the back side of the faces.

\begin{Verbatim}[commandchars=\\\{\},frame=single]
void PLUG_texture_color(
  inout vec4 texture_color,
  [const in samplerXxx texture,]
  const in vec4 tex_coord)
\end{Verbatim}

Calculate or modify the texture color.
This plug is available for texture effects. The second parameter of
this plug is special: its existence and type depends on the texture
node where it is used. For ShaderTexture, it's not available. For
other X3DTextureNode, its type corresponds to the GLSL sampler type
of given texture node: \texttt{sampler2D} for 2D textures,
\texttt{sampler3D} for 3D textures, \texttt{samplerCube} for cube
maps, and \texttt{sampler2DShadow} for \texttt{GeneratedShadowMap}

\begin{Verbatim}[commandchars=\\\{\},frame=single]
void PLUG_texture_apply(
  inout vec4 fragment_color,
  const in vec3 normal_eye)
\end{Verbatim}

Apply a texture color.

\begin{Verbatim}[commandchars=\\\{\},frame=single]
void PLUG_fog_apply(
  inout vec4 fragment_color,
  const vec3 normal_eye_fragment)
\end{Verbatim}

Apply the fog effect. This plug is called after
the \texttt{texture\_apply}, because you usually want to apply the fog
to the final (textured) fragment color.

\begin{Verbatim}[commandchars=\\\{\},frame=single]
void PLUG_fragment_end(
  const in vec4 fragment_color)
\end{Verbatim}

Do the final processing of the fragment. This is called after applying
both textures and fog, and cannot modify the fragment color anymore.
This is useful for operations like alpha-testing the fragment.

\section{Complete example models}

Examples are available inside our engine demo models on
\myhref{http://vrmlengine.sourceforge.net/demo\_models.php}{http://vrmlengine.sourceforge.net/demo_models.php}.
The relevant demos are mostly inside the \texttt{compositing\_shaders}
subdirectory, also the \texttt{water} subdirectory contains
the water implementation using our effects.
You can checkout them from SVN, or just browse through a web browser,
using the URL
\myhref{https://vrmlengine.svn.sourceforge.net/svnroot/vrmlengine/trunk/demo\_models/compositing\_shaders/}{https://vrmlengine.svn.sourceforge.net/svnroot/vrmlengine/trunk/demo_models/compositing_shaders/}.

You can open the examples using any of our engine tools,
like \texttt{view3dscene} from
\myhref{http://vrmlengine.sourceforge.net/view3dscene.php}{http://vrmlengine.sourceforge.net/view3dscene.php}.

You can run \texttt{view3dscene} with \texttt{--debug-log-shaders} command-line
option. Output (stdout) will show you the final shader code generated,
and also the OpenGL log after linking shaders.
Be sure to redirect the output to a file, and you may want to test it first
with a simple scene with one shape --- as the output may be quite large.
This is a useful way to learn about our shader rendering internals.

Another useful option to try in \texttt{view3dscene} is to switch to
\textit{View $->$ Shaders $->$ Enable For Everything} mode.
This will force shader rendering for all the shapes,
while by default we use shader rendering only for the shapes that
require particular effects (shaders by \texttt{ComposedShader}, effects
described in this paper, shadow maps and such).
Forcing shader rendering for everything allows to visually see
how our shaders implement the whole X3D lighting and texturing model.
It also makes the whole lighting calculation per-pixel, resulting
in particular in beautiful light spot highlights.

Development notes: only the SVN demo\_models contain
compositing\_shaders subdirectory.
You also have to use view3dscene from SVN or nightly builds,
see \myhref{http://michalis.ii.uni.wroc.pl/vrmlengine-snapshots/}{http://michalis.ii.uni.wroc.pl/vrmlengine-snapshots/}.

\bibliographystyle{acmsiggraph}
\nocite{*}
\bibliography{compositing_shaders}

\end{document}
